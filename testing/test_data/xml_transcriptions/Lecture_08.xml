<?xml version="1.0" encoding="UTF-8"?>
<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\documentclass[11pt]{article}\usepackage{aahomework}\usepackage{mathtools}%&#10;\usepackage{subcaption}\usepackage{epstopdf}\usepackage{float}\usepackage{%&#10;xcolor}\usepackage{parskip}\tikzstyle{blk}=[circle,innersep=0pt,minimumsize=4%&#10;pt,draw,fill=black,linewidth=0.8pt]\tikzstyle{blanknode}=[circle,innersep=3pt,%&#10;minimumsize=8pt,draw,linewidth=0.8pt]\par&#10;\geometry{letterpaper,textwidth=17cm%&#10;,textheight=22cm}\par&#10;\usetikzlibrary{arrows}\usetikzlibrary{plotmarks}\par&#10;%&#10;\par&#10;\par&#10;\begin{document}&#10;\par&#10;\@@section{section}{Sx1}{}{}{}{Recall}&#10;\par&#10; Matrix Algebra: addition, subtraction, scalar multiplication, and \emph{%&#10;matrix multiplication}.&#10;\par&#10; Matrix multiplication can be thought of in several ways:&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{B}$&amp;$\displaystyle=\begin{bmatrix}&amp;%&#10;&amp;\\&#10;\leftarrow&amp;i^{\text{th}}&amp;\rightarrow\\&#10;&amp;&amp;\end{bmatrix}\begin{bmatrix}&amp;\uparrow&amp;\\&#10;&amp;j^{\text{th}}&amp;\\&#10;&amp;\downarrow&amp;\end{bmatrix}$\\&#10;$$&amp;$\displaystyle=\text{Linear combination of columns of $\mathbf{A}$}$\\&#10;$$&amp;$\displaystyle=\text{Linear combination of rows of $\mathbf{B}$}$&#10;\par&#10; For $\mathbf{A}\mathbf{B}$ to be defined, the number of columns of $%&#10;\mathbf{A}$ must equal the number of rows of $\mathbf{B}$. If $\mathbf{A}_{m%&#10;\times n}$ and $\mathbf{B}_{n\times p}$, then their product $\mathbf{C}=%&#10;\mathbf{A}\mathbf{B}$ is defined and&#10;\@@amsalign$\displaystyle\mathbf{C}=(\mathbf{A}\mathbf{B})_{m\times p}$&#10;It is important to note a few points:&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$\mathbf{B}\mathbf{A}$ need not be even defined. Thus $\mathbf{A}\mathbf{B}%&#10;\neq\mathbf{B}\mathbf{A}$.&#10;\par&#10; Consider for example&#10;\@@amsalign$\displaystyle\mathbf{A}_{3\times 4}\qquad\mathbf{B}_{4\times 5}%&#10;\qquad(\mathbf{A}\mathbf{B})_{3\times 5}$&#10;But $\mathbf{B}\mathbf{A}$ is not defined.&#10;}&#10;\enumerate@item{&#10;In some cases, both $\mathbf{A}\mathbf{B}$ and $\mathbf{B}\mathbf{A}$ are %&#10;defined. However, it is still not the case that they are equal. Suppose we %&#10;have two matrices, $\mathbf{A}_{3\times 2}$ and $\mathbf{B}_{2\times 3}$. Then%&#10; the dimensions of their products will be given by&#10;\@@amsalign$\displaystyle(\mathbf{A}\mathbf{B})_{3\times 3}\neq(\mathbf{B}%&#10;\mathbf{A})_{2\times 2}$&#10;since the dimensions do not match, the matrices cannot be equal.&#10;}&#10;\enumerate@item{&#10;Finally, even if $\mathbf{A}\mathbf{B}$ and $\mathbf{B}\mathbf{A}$ are of the %&#10;same size, they still may be unequal. Suppose we have two matrices, $\mathbf{A%&#10;}_{2\times 2}$ and $\mathbf{B}_{2\times 2}$. Both of their products will be of%&#10; size $2\times 2$, yet they may not be equal:&#10;\@@amsalign$\displaystyle\begin{pmatrix}1&amp;2\\&#10;3&amp;4\end{pmatrix}\begin{pmatrix}0&amp;7\\&#10;1&amp;2\end{pmatrix}\neq\begin{pmatrix}0&amp;7\\&#10;1&amp;2\end{pmatrix}\begin{pmatrix}1&amp;2\\&#10;3&amp;4\end{pmatrix}$&#10;this is most easily determined by comparing the the top-left entry of both %&#10;products: $2$ for the first matrix, and $21$ for the second.&#10;}&#10;\end{enumerate}&#10;We can see then that matrix multiplication is fundamentally a non-commutative %&#10;operation. This is very different from the kind of algebra that we are %&#10;familiar with. There are several other instances where matrix multiplication %&#10;leads to counterintuitive results.&#10;\par&#10;\begin{enumerate}&#10;\enumerate@item{&#10;Consider $(A+B)^{2}$. We normally write $(A+B)^{2}=A^{2}+2AB+B^{2}$ for %&#10;numbers, but note that for matrices,&#10;\@@amsalign$\displaystyle(\mathbf{A}+\mathbf{B})^{2}$&amp;$\displaystyle=(\mathbf{%&#10;A}+\mathbf{B})(\mathbf{A}+\mathbf{B})$\\&#10;$$&amp;$\displaystyle=\mathbf{A}(\mathbf{A}+\mathbf{B})+\mathbf{B}(\mathbf{A}+%&#10;\mathbf{B})$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\mathbf{A}+\mathbf{A}\mathbf{B}+\mathbf{B}\mathbf{%&#10;A}+\mathbf{B}\mathbf{B}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{2}+\mathbf{A}\mathbf{B}+\mathbf{B}\mathbf{A}+%&#10;\mathbf{B}^{2}$&#10;As we have just found, $\mathbf{A}\mathbf{B}$ need not be equal to $\mathbf{B}%&#10;\mathbf{A}$, so we cannot write that $\mathbf{A}\mathbf{B}+\mathbf{B}\mathbf{A%&#10;}=2\mathbf{A}\mathbf{B}$.&#10;\par&#10;}&#10;\enumerate@item{&#10;\par&#10; Consider also the normally valid identity&#10;\@@amsalign$\displaystyle A^{2}-B^{2}$&amp;$\displaystyle=(A-B)(A+B)$&#10;{This identity need not hold for matrices.}&#10;}&#10;\enumerate@item{&#10;We regularly make use of the fact that if $AB=0$, then one of $A$ or $B$ (or %&#10;both) are zero. This is a fundamental property of what are called \emph{%&#10;integral domains}. Does this hold for matrices?&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0\\&#10;0&amp;2\end{bmatrix}=\begin{bmatrix}0&amp;0\\&#10;0&amp;0\end{bmatrix}$&#10;The two matrices on the left are non-zero matrices, yet their product is the %&#10;zero matrix. Thus $\mathbf{A}\mathbf{B}=0$ does not imply that one of $\mathbf%&#10;{A}$ or $\mathbf{B}$ are zero&#10;}&#10;\end{enumerate}&#10;\par&#10;\@@section{section}{Sx2}{}{}{}{Matrix Equations}&#10;We have discussed previously, and become comfortable with the notion that a %&#10;system of linear equations can be expressed as a so-called augmented matrix:&#10;\@@amsalign$\displaystyle\left.\begin{aligned}\displaystyle x+3y&amp;\displaystyle%&#10;=5\\&#10;\displaystyle 2x-3y&amp;\displaystyle=7\end{aligned}\right\}\Leftrightarrow\left[%&#10;\begin{matrix}[rr|r]1&amp;3&amp;5\\&#10;2&amp;-3&amp;7\end{matrix}\right]$&#10;Now however, we can also write this system as&#10;\@@amsalign$\displaystyle\left.\begin{aligned}\displaystyle x+3y&amp;\displaystyle%&#10;=5\\&#10;\displaystyle 2x-3y&amp;\displaystyle=7\end{aligned}\right\}\Longrightarrow\begin{%&#10;bmatrix}[rr]1&amp;3\\&#10;2&amp;-3\end{bmatrix}\begin{bmatrix}x\\&#10;y\end{bmatrix}=\begin{bmatrix}5\\&#10;7\end{bmatrix}$&#10;This form should look very similar: $\mathbf{A}\vec{x}=\vec{b}$. That is, this%&#10; looks like a regular linear equation.&#10;\par&#10;\@@section{section}{Sx3}{}{}{}{Special Types of Matrices}&#10;There are many special types of matrices.&#10;\@@section{subsection}{Sx3.SSx1}{}{}{}{Square Matrices: $(n\times n)$}&#10;\par&#10;\begin{enumerate}&#10;\enumerate@item{&#10;Diagonal matrix:&#10;\par&#10; A {diagonal matrix} has non-zero entries only on its {main diagonal}.&#10;\@@amsalign$\displaystyle\begin{bmatrix}d_{1}&amp;0&amp;\cdots&amp;0\\&#10;0&amp;d_{2}&amp;\cdots&amp;0\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;0&amp;0&amp;\cdots&amp;d_{n}\end{bmatrix}$&amp;$$&amp;$\displaystyle a_{i,j}=0\text{ if $i\neq j$}%&#10;$&#10;}&#10;\enumerate@item{&#10;Upper triangular matrix&#10;\par&#10; An {upper triangular} matrix has only zero entries below its main %&#10;diagonal.&#10;\@@amsalign$\displaystyle\begin{bmatrix}u_{1,1}&amp;u_{1,2}&amp;\cdots&amp;u_{1,n}\\&#10;0&amp;u_{2,2}&amp;\cdots&amp;u_{2,n}\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;0&amp;0&amp;\cdots&amp;u_{n,n}\end{bmatrix}$&amp;$$&amp;$\displaystyle u_{i,j}=0\text{ if $i&lt;j$}$&#10;}&#10;\enumerate@item{&#10;Lower triangular matrix&#10;\par&#10; A {lower triangular} matrix has only zero entries above its main %&#10;diagonal.&#10;\@@amsalign$\displaystyle\begin{bmatrix}l_{1,1}&amp;0&amp;\cdots&amp;0\\&#10;l_{2,1}&amp;l_{2,2}&amp;\cdots&amp;0\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;l_{n,1}&amp;l_{n,2}&amp;\cdots&amp;l_{n,n}\end{bmatrix}$&amp;$$&amp;$\displaystyle u_{i,j}=0\text{%&#10; if $i&gt;j$}$&#10;}&#10;\enumerate@item{&#10;Identity matrix ($\mathbf{I}_{n}$)&#10;\par&#10; The {identity matrix} is a diagonal matrix with $1$'s on all of its %&#10;diagonal entries.&#10;\@@amsalign$\displaystyle\mathbf{I}_{n}=\begin{bmatrix}1&amp;0&amp;\cdots&amp;0\\&#10;0&amp;1&amp;\cdots&amp;0\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;0&amp;0&amp;\cdots&amp;1\end{bmatrix}$&#10;\par&#10; The identity matrix is very important because multiplying any square $n%&#10;\times n$ matrix $\mathbf{A}$ by the identity matrix $\mathbf{I}_{n}$ will %&#10;yield $\mathbf{A}$:&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{I}_{n}=\mathbf{I}_{n}\mathbf{A}=%&#10;\mathbf{A}$&#10;In this way, $\mathbf{I}_{n}$ in $n\times n$ matrices is like the number $%&#10;\mathbf{1}$ in the real numbers.&#10;}&#10;\enumerate@item{&#10;Permutation matrix&#10;\par&#10; A {permutation} matrix is formed by permuting (swapping) the rows of the%&#10; identity matrix.&#10;\par&#10;\@@amsalign$\displaystyle\begin{bmatrix}0&amp;0&amp;1\\&#10;1&amp;0&amp;0\\&#10;0&amp;1&amp;0\end{bmatrix}\begin{bmatrix}x\\&#10;y\\&#10;z\end{bmatrix}=\begin{bmatrix}z\\&#10;x\\&#10;y\end{bmatrix}$&#10;\par&#10;{Remark:} The set of all permutation matrices of a given size $n$ is %&#10;called $S_{n}$. For example:&#10;\@@amsalign$\displaystyle S_{3}=\left\{\mathbf{A}_{3\times 3}|\text{$\mathbf{A%&#10;}$ is a permutation matrix}\right\}$&#10;Note that taking any two elements in $S_{3}$ and multiplying them together %&#10;yields another element in $S_{3}$. That is, $S_{3}$ is closed under %&#10;multiplication. The name for this type of structure is a group. One feature is%&#10; that $|S_{3}|=3!=6$. There are $n!$ permutation matrices of a given size $n$.&#10;}&#10;\end{enumerate}&#10;\par&#10;\@@section{subsection}{Sx3.SSx2}{}{}{}{Transpose: $\mathbf{A}^{T}$}&#10;\par&#10; Transposition is an operation unique to matrices. Let $\mathbf{A}_{m%&#10;\times n}$ be a matrix (not necessarily square). Then we define the {transpose%&#10; of $\mathbf{A}$}, written as $\mathbf{A}^{T}$, by letting its columns be the %&#10;rows of $\mathbf{A}$.&#10;{\Large\@@amsalign$\displaystyle\mathbf{A}_{m\times n}\Rightarrow\mathbf{A}^{T%&#10;}_{n\times m}$}&#10;For example,&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}1&amp;2&amp;3\\&#10;4&amp;5&amp;6\end{bmatrix}$&amp;$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\begin{%&#10;bmatrix}1&amp;4\\&#10;2&amp;5\\&#10;3&amp;6\end{bmatrix}$&#10;\par&#10; Some properties of the transpose include:&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$(\mathbf{A}+\mathbf{B})^{T}=\mathbf{A}^{T}+\mathbf{B}^{T}$&#10;}&#10;\enumerate@item{&#10;$(k\mathbf{A})^{T}=k\mathbf{A}^{T}$&#10;}&#10;\enumerate@item{&#10;$\Big(\mathbf{A}\mathbf{B}\Big)^{T}=\mathbf{B}^{T}\mathbf{A}^{T}$&#10;}&#10;\enumerate@item{&#10;$\Big(\mathbf{A}^{T}\Big)^{T}=\mathbf{A}$&#10;}&#10;\end{enumerate}&#10;\par&#10;\@@section{subsection}{Sx3.SSx3}{}{}{}{Symmetric Matrices}&#10;Transposition allows us to define two important types of matrices:&#10;\par&#10;\begin{itemize}&#10;\itemize@item{&#10;$\mathbf{A}$ is {symmetric} if $\mathbf{A}^{T}=\mathbf{A}$.&#10;}&#10;\itemize@item{&#10;$\mathbf{A}$ is {skew-symmetric} if $\mathbf{A}^{T}=-\mathbf{A}$.&#10;}&#10;\end{itemize}&#10;Examining these definitions, it is easy to see that a symmetric or skew-%&#10;symmetric matrix must be an $n\times n$ matrix, even though transposition is %&#10;defined for all matrices.&#10;\par&#10; Let us examine some features of symmetric matrices. Consider $\mathbf{B}%&#10;=\mathbf{A}+\mathbf{A}^{T}$ with $\mathbf{A}$ an $n\times n$ matrix. Is $%&#10;\mathbf{B}$ symmetric?&#10;\par&#10;\@@amsalign$\displaystyle\mathbf{B}$&amp;$\displaystyle=\Big(\mathbf{A}+%&#10;\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}+\Big(\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}+\mathbf{A}$\\&#10;$$&amp;$\displaystyle=\mathbf{B}$&#10;Therefore $\mathbf{B}$ is symmetric.&#10;\par&#10; Consider $\mathbf{C}=\mathbf{A}-\mathbf{A}^{T}$. Is $\mathbf{C}$ %&#10;symmetric / skew-symmetric?&#10;\par&#10;\@@amsalign$\displaystyle\mathbf{C}^{T}$&amp;$\displaystyle=\Big(\mathbf{A}-%&#10;\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}-\Big(\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}-\mathbf{A}$\\&#10;$$&amp;$\displaystyle=-\mathbf{C}$&#10;Thus $\mathbf{C}$ is skew-symmetric.&#10;\par&#10; It is natural to wonder what types of matrices are symmetric or skew-%&#10;symmetric. Consider for example:&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}1&amp;2\\&#10;3&amp;4\end{bmatrix}$&amp;$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\begin{bmatrix}%&#10;1&amp;3\\&#10;2&amp;4\end{bmatrix}$&#10;Note that $\mathbf{A}^{T}\neq\mathbf{A}$ and $\mathbf{A}^{T}\neq-\mathbf{A}$. %&#10;Thus $\mathbf{A}$ is neither symmetric nor skew-symmetric.&#10;\par&#10; An important observation can be made by asking: what kinds of matrices %&#10;can in general be skew-symmetric?&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}a&amp;b\\&#10;c&amp;d\end{bmatrix}$&amp;$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\begin{bmatrix}%&#10;a&amp;c\\&#10;b&amp;d\end{bmatrix}$&#10;For $\mathbf{A}$ to be skew-symmetric,&#10;\@@amsalign$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=-\mathbf{A}$\\&#10;$\displaystyle\begin{bmatrix}a&amp;c\\&#10;b&amp;d\end{bmatrix}$&amp;$\displaystyle=\begin{bmatrix}-a&amp;-b\\&#10;-c&amp;-d\end{bmatrix}$&#10;Thus $a=-a$ and $d=-d$. This can only be true when $a=d=0$. This can be %&#10;generalized to any size $n$:&#10;\par&#10; Proposition: If $\mathbf{A}$ is a skew-symmetric matrix then the %&#10;diagonal entries must be $0$.&#10;\par&#10;\par&#10; Here is a question to consider. If you recall, for any function $f(%&#10;x)$, it is possible to write $f(x)$ as the sum of an even and an odd function.%&#10; Is a similar result true for matrices?&#10;\@@amsalign$\displaystyle f(x)$&amp;$\displaystyle=\text{even function}+\text{odd %&#10;function}$\\&#10;$$\\&#10;$\displaystyle\mathbf{A}_{n\times n}$&amp;$\displaystyle=\text{symmetric matrix}+%&#10;\text{skew-symmetric matrix}$&#10;If it were true that this is the case, then we would want&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\mathbf{B}+\mathbf{C}$&#10;where $\mathbf{B}$ is symmetric and $\mathbf{C}$ is skew-symmetric. Then&#10;\@@amsalign$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\mathbf{B}^{T}+\mathbf%&#10;{C}^{T}=\mathbf{B}-\mathbf{C}$&#10;Adding and subtracting these two equations, we obtain the two equations&#10;\@@amsalign$\displaystyle\frac{\mathbf{A}+\mathbf{A}^{T}}{2}$&amp;$\displaystyle=%&#10;\mathbf{B}$&amp;$\displaystyle\frac{\mathbf{A}-\mathbf{A}^{T}}{2}$&amp;$\displaystyle=%&#10;\mathbf{C}$&#10;What is the use in this result? Apart from it being interesting that this type%&#10; of expression of an arbitrary matrix is always possible, the advantage of %&#10;this decomposition is that if, perhaps, we can gain insight into features of %&#10;symmetric and skew-symmetric matrices, then we can apply those understandings %&#10;to other matrices using the decomposition.&#10;\par&#10;\end{document}" display="block">
  <mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>k</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>4</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>f</mi>
      <mi>i</mi>
      <mi>l</mi>
      <mi>l</mi>
      <mo>=</mo>
      <mi>b</mi>
      <mi>l</mi>
      <mi>a</mi>
      <mi>c</mi>
      <mi>k</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mi>n</mi>
    <mi>o</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>3</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\geometry</mtext>
    </merror>
    <mi>l</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>p</mi>
    <mi>a</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>w</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mo>=</mo>
    <mn>17</mn>
    <mi>c</mi>
    <mi>m</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>h</mi>
    <mi>t</mi>
    <mo>=</mo>
    <mn>22</mn>
    <mi>c</mi>
    <mi>m</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>a</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>w</mi>
    <mi>s</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>p</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>k</mi>
    <mi>s</mi>
    <mtext xml:id="Sx1">RecallMatrix Algebra: addition, subtraction, scalar multiplication, and matrix multiplication.Matrix multiplication can be thought of in several ways:𝐀𝐁=[←ith→]⁢[↑jth↓]𝐀𝐁=[←ith→]⁢[↑jth↓]=Linear combination of columns of 𝐀=Linear combination of columns of 𝐀=Linear combination of rows of 𝐁=Linear combination of rows of 𝐁For 𝐀𝐁 to be defined, the number of columns of 𝐀 must equal the number of rows of 𝐁. If 𝐀m×n and 𝐁n×p, then their product 𝐂=𝐀𝐁 is defined and𝐂=(𝐀𝐁)m×p𝐂=(𝐀𝐁)m×pIt is important to note a few points:1.𝐁𝐀 need not be even defined. Thus 𝐀𝐁≠𝐁𝐀.Consider for example𝐀3×4  𝐁4×5  (𝐀𝐁)3×5𝐀3×4  𝐁4×5  (𝐀𝐁)3×5But 𝐁𝐀 is not defined.2.In some cases, both 𝐀𝐁 and 𝐁𝐀 are defined. However, it is still not the case that they are equal. Suppose we have two matrices, 𝐀3×2 and 𝐁2×3. Then the dimensions of their products will be given by(𝐀𝐁)3×3≠(𝐁𝐀)2×2(𝐀𝐁)3×3≠(𝐁𝐀)2×2since the dimensions do not match, the matrices cannot be equal.3.Finally, even if 𝐀𝐁 and 𝐁𝐀 are of the same size, they still may be unequal. Suppose we have two matrices, 𝐀2×2 and 𝐁2×2. Both of their products will be of size 2×2, yet they may not be equal:(1234)⁢(0712)≠(0712)⁢(1234)(1234)⁢(0712)≠(0712)⁢(1234)this is most easily determined by comparing the the top-left entry of both products: 2 for the first matrix, and 21 for the second.We can see then that matrix multiplication is fundamentally a non-commutative operation. This is very different from the kind of algebra that we are familiar with. There are several other instances where matrix multiplication leads to counterintuitive results.1.Consider (A+B)2. We normally write (A+B)2=A2+2⁢A⁢B+B2 for numbers, but note that for matrices,(𝐀+𝐁)2=(𝐀+𝐁)⁢(𝐀+𝐁)(𝐀+𝐁)2=(𝐀+𝐁)⁢(𝐀+𝐁)=𝐀⁢(𝐀+𝐁)+𝐁⁢(𝐀+𝐁)=𝐀⁢(𝐀+𝐁)+𝐁⁢(𝐀+𝐁)=𝐀𝐀+𝐀𝐁+𝐁𝐀+𝐁𝐁=𝐀𝐀+𝐀𝐁+𝐁𝐀+𝐁𝐁=𝐀2+𝐀𝐁+𝐁𝐀+𝐁2=𝐀2+𝐀𝐁+𝐁𝐀+𝐁2As we have just found, 𝐀𝐁 need not be equal to 𝐁𝐀, so we cannot write that 𝐀𝐁+𝐁𝐀=2⁢𝐀𝐁.2.Consider also the normally valid identityA2-B2=(A-B)⁢(A+B)A2-B2=(A-B)⁢(A+B)This identity need not hold for matrices.3.We regularly make use of the fact that if A⁢B=0, then one of A or B (or both) are zero. This is a fundamental property of what are called integral domains. Does this hold for matrices?[1000]⁢[0002]=[0000][1000]⁢[0002]=[0000]The two matrices on the left are non-zero matrices, yet their product is the zero matrix. Thus 𝐀𝐁=0 does not imply that one of 𝐀 or 𝐁 are zeroMatrix EquationsWe have discussed previously, and become comfortable with the notion that a system of linear equations can be expressed as a so-called augmented matrix:x+3⁢y=52⁢x-3⁢y=7}⇔[[rr|r]1352-37]x+3⁢y=52⁢x-3⁢y=7}⇔[[rr|r]1352-37]Now however, we can also write this system asx+3⁢y=52⁢x-3⁢y=7}⟹[[r⁢r]⁢132-3][xy]=[57]x+3⁢y=52⁢x-3⁢y=7}⟹[[r⁢r]⁢132-3][xy]=[57]This form should look very similar: 𝐀⁢x→=b→. That is, this looks like a regular linear equation.Special Types of MatricesThere are many special types of matrices.Square Matrices: (n×n)1.Diagonal matrix:A diagonal matrix has non-zero entries only on its main diagonal.[d10⋯00d2⋯0⋮⋮⋱⋮00⋯dn][d10⋯00d2⋯0⋮⋮⋱⋮00⋯dn]ai,j=0⁢ if i≠jai,j=0⁢ if i≠j2.Upper triangular matrixAn upper triangular matrix has only zero entries below its main diagonal.[u1,1u1,2⋯u1,n0u2,2⋯u2,n⋮⋮⋱⋮00⋯un,n][u1,1u1,2⋯u1,n0u2,2⋯u2,n⋮⋮⋱⋮00⋯un,n]ui,j=0⁢ if i&lt;jui,j=0⁢ if i&lt;j3.Lower triangular matrixA lower triangular matrix has only zero entries above its main diagonal.[l1,10⋯0l2,1l2,2⋯0⋮⋮⋱⋮ln,1ln,2⋯ln,n][l1,10⋯0l2,1l2,2⋯0⋮⋮⋱⋮ln,1ln,2⋯ln,n]ui,j=0⁢ if i&gt;jui,j=0⁢ if i&gt;j4.Identity matrix (𝐈n)The identity matrix is a diagonal matrix with 1’s on all of its diagonal entries.𝐈n=[10⋯001⋯0⋮⋮⋱⋮00⋯1]𝐈n=[10⋯001⋯0⋮⋮⋱⋮00⋯1]The identity matrix is very important because multiplying any square n×n matrix 𝐀 by the identity matrix 𝐈n will yield 𝐀:𝐀𝐈n=𝐈n⁢𝐀=𝐀𝐀𝐈n=𝐈n⁢𝐀=𝐀In this way, 𝐈n in n×n matrices is like the number 𝟏 in the real numbers.5.Permutation matrixA permutation matrix is formed by permuting (swapping) the rows of the identity matrix.[001100010]⁢[xyz]=[zxy][001100010]⁢[xyz]=[zxy]Remark: The set of all permutation matrices of a given size n is called Sn. For example:S3={𝐀3×3|𝐀 is a permutation matrix}S3={𝐀3×3|𝐀 is a permutation matrix}Note that taking any two elements in S3 and multiplying them together yields another element in S3. That is, S3 is closed under multiplication. The name for this type of structure is a group. One feature is that |S3|=3!=6. There are n! permutation matrices of a given size n.Transpose: 𝐀TTransposition is an operation unique to matrices. Let 𝐀m×n be a matrix (not necessarily square). Then we define the transpose of 𝐀, written as 𝐀T, by letting its columns be the rows of 𝐀.𝐀m×n⇒𝐀n×mT𝐀m×n⇒𝐀n×mTFor example,𝐀=[123456]𝐀=[123456]𝐀T=[142536]𝐀T=[142536]Some properties of the transpose include:1.(𝐀+𝐁)T=𝐀T+𝐁T2.(k⁢𝐀)T=k⁢𝐀T3.(𝐀𝐁)T=𝐁T⁢𝐀T4.(𝐀T)T=𝐀Symmetric MatricesTransposition allows us to define two important types of matrices:•𝐀 is symmetric if 𝐀T=𝐀.•𝐀 is skew-symmetric if 𝐀T=-𝐀.Examining these definitions, it is easy to see that a symmetric or skew-symmetric matrix must be an n×n matrix, even though transposition is defined for all matrices.Let us examine some features of symmetric matrices. Consider 𝐁=𝐀+𝐀T with 𝐀 an n×n matrix. Is 𝐁 symmetric?𝐁=(𝐀+𝐀T)T𝐁=(𝐀+𝐀T)T=𝐀T+(𝐀T)T=𝐀T+(𝐀T)T=𝐀T+𝐀=𝐀T+𝐀=𝐁=𝐁Therefore 𝐁 is symmetric.Consider 𝐂=𝐀-𝐀T. Is 𝐂 symmetric / skew-symmetric?𝐂T=(𝐀-𝐀T)T𝐂T=(𝐀-𝐀T)T=𝐀T-(𝐀T)T=𝐀T-(𝐀T)T=𝐀T-𝐀=𝐀T-𝐀=-𝐂=-𝐂Thus 𝐂 is skew-symmetric.It is natural to wonder what types of matrices are symmetric or skew-symmetric. Consider for example:𝐀=[1234]𝐀=[1234]𝐀T=[1324]𝐀T=[1324]Note that 𝐀T≠𝐀 and 𝐀T≠-𝐀. Thus 𝐀 is neither symmetric nor skew-symmetric.An important observation can be made by asking: what kinds of matrices can in general be skew-symmetric?𝐀=[abcd]𝐀=[abcd]𝐀T=[acbd]𝐀T=[acbd]For 𝐀 to be skew-symmetric,𝐀T=-𝐀𝐀T=-𝐀[acbd]=[-a-b-c-d][acbd]=[-a-b-c-d]Thus a=-a and d=-d. This can only be true when a=d=0. This can be generalized to any size n:Proposition: If 𝐀 is a skew-symmetric matrix then the diagonal entries must be 0.Here is a question to consider. If you recall, for any function f⁢(x), it is possible to write f⁢(x) as the sum of an even and an odd function. Is a similar result true for matrices?f⁢(x)=even function+odd functionf⁢(x)=even function+odd function𝐀n×n=symmetric matrix+skew-symmetric matrix𝐀n×n=symmetric matrix+skew-symmetric matrixIf it were true that this is the case, then we would want𝐀=𝐁+𝐂𝐀=𝐁+𝐂where 𝐁 is symmetric and 𝐂 is skew-symmetric. Then𝐀T=𝐁T+𝐂T=𝐁-𝐂𝐀T=𝐁T+𝐂T=𝐁-𝐂Adding and subtracting these two equations, we obtain the two equations𝐀+𝐀T2=𝐁𝐀+𝐀T2=𝐁𝐀-𝐀T2=𝐂𝐀-𝐀T2=𝐂What is the use in this result? Apart from it being interesting that this type of expression of an arbitrary matrix is always possible, the advantage of this decomposition is that if, perhaps, we can gain insight into features of symmetric and skew-symmetric matrices, then we can apply those understandings to other matrices using the decomposition.</mtext>
  </mrow>
</math>
