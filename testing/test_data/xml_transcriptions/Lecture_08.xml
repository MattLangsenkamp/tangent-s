<?xml version="1.0" encoding="UTF-8"?>
<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\documentclass[11pt]{article}\usepackage{aahomework}\usepackage{mathtools}%&#10;\usepackage{subcaption}\usepackage{epstopdf}\usepackage{float}\usepackage{%&#10;xcolor}\usepackage{parskip}\tikzstyle{blk}=[circle,innersep=0pt,minimumsize=4%&#10;pt,draw,fill=black,linewidth=0.8pt]\tikzstyle{blanknode}=[circle,innersep=3pt,%&#10;minimumsize=8pt,draw,linewidth=0.8pt]\par&#10;\geometry{letterpaper,textwidth=17cm%&#10;,textheight=22cm}\par&#10;\usetikzlibrary{arrows}\usetikzlibrary{plotmarks}\par&#10;%&#10;\par&#10;\par&#10;\begin{document}&#10;\par&#10;\@@section{section}{Sx1}{}{}{}{Recall}&#10;\par&#10; Matrix Algebra: addition, subtraction, scalar multiplication, and \emph{%&#10;matrix multiplication}.&#10;\par&#10; Matrix multiplication can be thought of in several ways:&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{B}$&amp;$\displaystyle=\begin{bmatrix}&amp;%&#10;&amp;\\&#10;\leftarrow&amp;i^{\text{th}}&amp;\rightarrow\\&#10;&amp;&amp;\end{bmatrix}\begin{bmatrix}&amp;\uparrow&amp;\\&#10;&amp;j^{\text{th}}&amp;\\&#10;&amp;\downarrow&amp;\end{bmatrix}$\\&#10;$$&amp;$\displaystyle=\text{Linear combination of columns of $\mathbf{A}$}$\\&#10;$$&amp;$\displaystyle=\text{Linear combination of rows of $\mathbf{B}$}$&#10;\par&#10; For $\mathbf{A}\mathbf{B}$ to be defined, the number of columns of $%&#10;\mathbf{A}$ must equal the number of rows of $\mathbf{B}$. If $\mathbf{A}_{m%&#10;\times n}$ and $\mathbf{B}_{n\times p}$, then their product $\mathbf{C}=%&#10;\mathbf{A}\mathbf{B}$ is defined and&#10;\@@amsalign$\displaystyle\mathbf{C}=(\mathbf{A}\mathbf{B})_{m\times p}$&#10;It is important to note a few points:&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$\mathbf{B}\mathbf{A}$ need not be even defined. Thus $\mathbf{A}\mathbf{B}%&#10;\neq\mathbf{B}\mathbf{A}$.&#10;\par&#10; Consider for example&#10;\@@amsalign$\displaystyle\mathbf{A}_{3\times 4}\qquad\mathbf{B}_{4\times 5}%&#10;\qquad(\mathbf{A}\mathbf{B})_{3\times 5}$&#10;But $\mathbf{B}\mathbf{A}$ is not defined.&#10;}&#10;\enumerate@item{&#10;In some cases, both $\mathbf{A}\mathbf{B}$ and $\mathbf{B}\mathbf{A}$ are %&#10;defined. However, it is still not the case that they are equal. Suppose we %&#10;have two matrices, $\mathbf{A}_{3\times 2}$ and $\mathbf{B}_{2\times 3}$. Then%&#10; the dimensions of their products will be given by&#10;\@@amsalign$\displaystyle(\mathbf{A}\mathbf{B})_{3\times 3}\neq(\mathbf{B}%&#10;\mathbf{A})_{2\times 2}$&#10;since the dimensions do not match, the matrices cannot be equal.&#10;}&#10;\enumerate@item{&#10;Finally, even if $\mathbf{A}\mathbf{B}$ and $\mathbf{B}\mathbf{A}$ are of the %&#10;same size, they still may be unequal. Suppose we have two matrices, $\mathbf{A%&#10;}_{2\times 2}$ and $\mathbf{B}_{2\times 2}$. Both of their products will be of%&#10; size $2\times 2$, yet they may not be equal:&#10;\@@amsalign$\displaystyle\begin{pmatrix}1&amp;2\\&#10;3&amp;4\end{pmatrix}\begin{pmatrix}0&amp;7\\&#10;1&amp;2\end{pmatrix}\neq\begin{pmatrix}0&amp;7\\&#10;1&amp;2\end{pmatrix}\begin{pmatrix}1&amp;2\\&#10;3&amp;4\end{pmatrix}$&#10;this is most easily determined by comparing the the top-left entry of both %&#10;products: $2$ for the first matrix, and $21$ for the second.&#10;}&#10;\end{enumerate}&#10;We can see then that matrix multiplication is fundamentally a non-commutative %&#10;operation. This is very different from the kind of algebra that we are %&#10;familiar with. There are several other instances where matrix multiplication %&#10;leads to counterintuitive results.&#10;\par&#10;\begin{enumerate}&#10;\enumerate@item{&#10;Consider $(A+B)^{2}$. We normally write $(A+B)^{2}=A^{2}+2AB+B^{2}$ for %&#10;numbers, but note that for matrices,&#10;\@@amsalign$\displaystyle(\mathbf{A}+\mathbf{B})^{2}$&amp;$\displaystyle=(\mathbf{%&#10;A}+\mathbf{B})(\mathbf{A}+\mathbf{B})$\\&#10;$$&amp;$\displaystyle=\mathbf{A}(\mathbf{A}+\mathbf{B})+\mathbf{B}(\mathbf{A}+%&#10;\mathbf{B})$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\mathbf{A}+\mathbf{A}\mathbf{B}+\mathbf{B}\mathbf{%&#10;A}+\mathbf{B}\mathbf{B}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{2}+\mathbf{A}\mathbf{B}+\mathbf{B}\mathbf{A}+%&#10;\mathbf{B}^{2}$&#10;As we have just found, $\mathbf{A}\mathbf{B}$ need not be equal to $\mathbf{B}%&#10;\mathbf{A}$, so we cannot write that $\mathbf{A}\mathbf{B}+\mathbf{B}\mathbf{A%&#10;}=2\mathbf{A}\mathbf{B}$.&#10;\par&#10;}&#10;\enumerate@item{&#10;\par&#10; Consider also the normally valid identity&#10;\@@amsalign$\displaystyle A^{2}-B^{2}$&amp;$\displaystyle=(A-B)(A+B)$&#10;{This identity need not hold for matrices.}&#10;}&#10;\enumerate@item{&#10;We regularly make use of the fact that if $AB=0$, then one of $A$ or $B$ (or %&#10;both) are zero. This is a fundamental property of what are called \emph{%&#10;integral domains}. Does this hold for matrices?&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0\\&#10;0&amp;2\end{bmatrix}=\begin{bmatrix}0&amp;0\\&#10;0&amp;0\end{bmatrix}$&#10;The two matrices on the left are non-zero matrices, yet their product is the %&#10;zero matrix. Thus $\mathbf{A}\mathbf{B}=0$ does not imply that one of $\mathbf%&#10;{A}$ or $\mathbf{B}$ are zero&#10;}&#10;\end{enumerate}&#10;\par&#10;\@@section{section}{Sx2}{}{}{}{Matrix Equations}&#10;We have discussed previously, and become comfortable with the notion that a %&#10;system of linear equations can be expressed as a so-called augmented matrix:&#10;\@@amsalign$\displaystyle\left.\begin{aligned}\displaystyle x+3y&amp;\displaystyle%&#10;=5\\&#10;\displaystyle 2x-3y&amp;\displaystyle=7\end{aligned}\right\}\Leftrightarrow\left[%&#10;\begin{matrix}[rr|r]1&amp;3&amp;5\\&#10;2&amp;-3&amp;7\end{matrix}\right]$&#10;Now however, we can also write this system as&#10;\@@amsalign$\displaystyle\left.\begin{aligned}\displaystyle x+3y&amp;\displaystyle%&#10;=5\\&#10;\displaystyle 2x-3y&amp;\displaystyle=7\end{aligned}\right\}\Longrightarrow\begin{%&#10;bmatrix}[rr]1&amp;3\\&#10;2&amp;-3\end{bmatrix}\begin{bmatrix}x\\&#10;y\end{bmatrix}=\begin{bmatrix}5\\&#10;7\end{bmatrix}$&#10;This form should look very similar: $\mathbf{A}\vec{x}=\vec{b}$. That is, this%&#10; looks like a regular linear equation.&#10;\par&#10;\@@section{section}{Sx3}{}{}{}{Special Types of Matrices}&#10;There are many special types of matrices.&#10;\@@section{subsection}{Sx3.SSx1}{}{}{}{Square Matrices: $(n\times n)$}&#10;\par&#10;\begin{enumerate}&#10;\enumerate@item{&#10;Diagonal matrix:&#10;\par&#10; A {diagonal matrix} has non-zero entries only on its {main diagonal}.&#10;\@@amsalign$\displaystyle\begin{bmatrix}d_{1}&amp;0&amp;\cdots&amp;0\\&#10;0&amp;d_{2}&amp;\cdots&amp;0\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;0&amp;0&amp;\cdots&amp;d_{n}\end{bmatrix}$&amp;$$&amp;$\displaystyle a_{i,j}=0\text{ if $i\neq j$}%&#10;$&#10;}&#10;\enumerate@item{&#10;Upper triangular matrix&#10;\par&#10; An {upper triangular} matrix has only zero entries below its main %&#10;diagonal.&#10;\@@amsalign$\displaystyle\begin{bmatrix}u_{1,1}&amp;u_{1,2}&amp;\cdots&amp;u_{1,n}\\&#10;0&amp;u_{2,2}&amp;\cdots&amp;u_{2,n}\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;0&amp;0&amp;\cdots&amp;u_{n,n}\end{bmatrix}$&amp;$$&amp;$\displaystyle u_{i,j}=0\text{ if $i&lt;j$}$&#10;}&#10;\enumerate@item{&#10;Lower triangular matrix&#10;\par&#10; A {lower triangular} matrix has only zero entries above its main %&#10;diagonal.&#10;\@@amsalign$\displaystyle\begin{bmatrix}l_{1,1}&amp;0&amp;\cdots&amp;0\\&#10;l_{2,1}&amp;l_{2,2}&amp;\cdots&amp;0\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;l_{n,1}&amp;l_{n,2}&amp;\cdots&amp;l_{n,n}\end{bmatrix}$&amp;$$&amp;$\displaystyle u_{i,j}=0\text{%&#10; if $i&gt;j$}$&#10;}&#10;\enumerate@item{&#10;Identity matrix ($\mathbf{I}_{n}$)&#10;\par&#10; The {identity matrix} is a diagonal matrix with $1$'s on all of its %&#10;diagonal entries.&#10;\@@amsalign$\displaystyle\mathbf{I}_{n}=\begin{bmatrix}1&amp;0&amp;\cdots&amp;0\\&#10;0&amp;1&amp;\cdots&amp;0\\&#10;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\&#10;0&amp;0&amp;\cdots&amp;1\end{bmatrix}$&#10;\par&#10; The identity matrix is very important because multiplying any square $n%&#10;\times n$ matrix $\mathbf{A}$ by the identity matrix $\mathbf{I}_{n}$ will %&#10;yield $\mathbf{A}$:&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{I}_{n}=\mathbf{I}_{n}\mathbf{A}=%&#10;\mathbf{A}$&#10;In this way, $\mathbf{I}_{n}$ in $n\times n$ matrices is like the number $%&#10;\mathbf{1}$ in the real numbers.&#10;}&#10;\enumerate@item{&#10;Permutation matrix&#10;\par&#10; A {permutation} matrix is formed by permuting (swapping) the rows of the%&#10; identity matrix.&#10;\par&#10;\@@amsalign$\displaystyle\begin{bmatrix}0&amp;0&amp;1\\&#10;1&amp;0&amp;0\\&#10;0&amp;1&amp;0\end{bmatrix}\begin{bmatrix}x\\&#10;y\\&#10;z\end{bmatrix}=\begin{bmatrix}z\\&#10;x\\&#10;y\end{bmatrix}$&#10;\par&#10;{Remark:} The set of all permutation matrices of a given size $n$ is %&#10;called $S_{n}$. For example:&#10;\@@amsalign$\displaystyle S_{3}=\left\{\mathbf{A}_{3\times 3}|\text{$\mathbf{A%&#10;}$ is a permutation matrix}\right\}$&#10;Note that taking any two elements in $S_{3}$ and multiplying them together %&#10;yields another element in $S_{3}$. That is, $S_{3}$ is closed under %&#10;multiplication. The name for this type of structure is a group. One feature is%&#10; that $|S_{3}|=3!=6$. There are $n!$ permutation matrices of a given size $n$.&#10;}&#10;\end{enumerate}&#10;\par&#10;\@@section{subsection}{Sx3.SSx2}{}{}{}{Transpose: $\mathbf{A}^{T}$}&#10;\par&#10; Transposition is an operation unique to matrices. Let $\mathbf{A}_{m%&#10;\times n}$ be a matrix (not necessarily square). Then we define the {transpose%&#10; of $\mathbf{A}$}, written as $\mathbf{A}^{T}$, by letting its columns be the %&#10;rows of $\mathbf{A}$.&#10;{\Large\@@amsalign$\displaystyle\mathbf{A}_{m\times n}\Rightarrow\mathbf{A}^{T%&#10;}_{n\times m}$}&#10;For example,&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}1&amp;2&amp;3\\&#10;4&amp;5&amp;6\end{bmatrix}$&amp;$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\begin{%&#10;bmatrix}1&amp;4\\&#10;2&amp;5\\&#10;3&amp;6\end{bmatrix}$&#10;\par&#10; Some properties of the transpose include:&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$(\mathbf{A}+\mathbf{B})^{T}=\mathbf{A}^{T}+\mathbf{B}^{T}$&#10;}&#10;\enumerate@item{&#10;$(k\mathbf{A})^{T}=k\mathbf{A}^{T}$&#10;}&#10;\enumerate@item{&#10;$\Big(\mathbf{A}\mathbf{B}\Big)^{T}=\mathbf{B}^{T}\mathbf{A}^{T}$&#10;}&#10;\enumerate@item{&#10;$\Big(\mathbf{A}^{T}\Big)^{T}=\mathbf{A}$&#10;}&#10;\end{enumerate}&#10;\par&#10;\@@section{subsection}{Sx3.SSx3}{}{}{}{Symmetric Matrices}&#10;Transposition allows us to define two important types of matrices:&#10;\par&#10;\begin{itemize}&#10;\itemize@item{&#10;$\mathbf{A}$ is {symmetric} if $\mathbf{A}^{T}=\mathbf{A}$.&#10;}&#10;\itemize@item{&#10;$\mathbf{A}$ is {skew-symmetric} if $\mathbf{A}^{T}=-\mathbf{A}$.&#10;}&#10;\end{itemize}&#10;Examining these definitions, it is easy to see that a symmetric or skew-%&#10;symmetric matrix must be an $n\times n$ matrix, even though transposition is %&#10;defined for all matrices.&#10;\par&#10; Let us examine some features of symmetric matrices. Consider $\mathbf{B}%&#10;=\mathbf{A}+\mathbf{A}^{T}$ with $\mathbf{A}$ an $n\times n$ matrix. Is $%&#10;\mathbf{B}$ symmetric?&#10;\par&#10;\@@amsalign$\displaystyle\mathbf{B}$&amp;$\displaystyle=\Big(\mathbf{A}+%&#10;\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}+\Big(\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}+\mathbf{A}$\\&#10;$$&amp;$\displaystyle=\mathbf{B}$&#10;Therefore $\mathbf{B}$ is symmetric.&#10;\par&#10; Consider $\mathbf{C}=\mathbf{A}-\mathbf{A}^{T}$. Is $\mathbf{C}$ %&#10;symmetric / skew-symmetric?&#10;\par&#10;\@@amsalign$\displaystyle\mathbf{C}^{T}$&amp;$\displaystyle=\Big(\mathbf{A}-%&#10;\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}-\Big(\mathbf{A}^{T}\Big)^{T}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}^{T}-\mathbf{A}$\\&#10;$$&amp;$\displaystyle=-\mathbf{C}$&#10;Thus $\mathbf{C}$ is skew-symmetric.&#10;\par&#10; It is natural to wonder what types of matrices are symmetric or skew-%&#10;symmetric. Consider for example:&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}1&amp;2\\&#10;3&amp;4\end{bmatrix}$&amp;$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\begin{bmatrix}%&#10;1&amp;3\\&#10;2&amp;4\end{bmatrix}$&#10;Note that $\mathbf{A}^{T}\neq\mathbf{A}$ and $\mathbf{A}^{T}\neq-\mathbf{A}$. %&#10;Thus $\mathbf{A}$ is neither symmetric nor skew-symmetric.&#10;\par&#10; An important observation can be made by asking: what kinds of matrices %&#10;can in general be skew-symmetric?&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}a&amp;b\\&#10;c&amp;d\end{bmatrix}$&amp;$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\begin{bmatrix}%&#10;a&amp;c\\&#10;b&amp;d\end{bmatrix}$&#10;For $\mathbf{A}$ to be skew-symmetric,&#10;\@@amsalign$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=-\mathbf{A}$\\&#10;$\displaystyle\begin{bmatrix}a&amp;c\\&#10;b&amp;d\end{bmatrix}$&amp;$\displaystyle=\begin{bmatrix}-a&amp;-b\\&#10;-c&amp;-d\end{bmatrix}$&#10;Thus $a=-a$ and $d=-d$. This can only be true when $a=d=0$. This can be %&#10;generalized to any size $n$:&#10;\par&#10; Proposition: If $\mathbf{A}$ is a skew-symmetric matrix then the %&#10;diagonal entries must be $0$.&#10;\par&#10;\par&#10; Here is a question to consider. If you recall, for any function $f(%&#10;x)$, it is possible to write $f(x)$ as the sum of an even and an odd function.%&#10; Is a similar result true for matrices?&#10;\@@amsalign$\displaystyle f(x)$&amp;$\displaystyle=\text{even function}+\text{odd %&#10;function}$\\&#10;$$\\&#10;$\displaystyle\mathbf{A}_{n\times n}$&amp;$\displaystyle=\text{symmetric matrix}+%&#10;\text{skew-symmetric matrix}$&#10;If it were true that this is the case, then we would want&#10;\@@amsalign$\displaystyle\mathbf{A}$&amp;$\displaystyle=\mathbf{B}+\mathbf{C}$&#10;where $\mathbf{B}$ is symmetric and $\mathbf{C}$ is skew-symmetric. Then&#10;\@@amsalign$\displaystyle\mathbf{A}^{T}$&amp;$\displaystyle=\mathbf{B}^{T}+\mathbf%&#10;{C}^{T}=\mathbf{B}-\mathbf{C}$&#10;Adding and subtracting these two equations, we obtain the two equations&#10;\@@amsalign$\displaystyle\frac{\mathbf{A}+\mathbf{A}^{T}}{2}$&amp;$\displaystyle=%&#10;\mathbf{B}$&amp;$\displaystyle\frac{\mathbf{A}-\mathbf{A}^{T}}{2}$&amp;$\displaystyle=%&#10;\mathbf{C}$&#10;What is the use in this result? Apart from it being interesting that this type%&#10; of expression of an arbitrary matrix is always possible, the advantage of %&#10;this decomposition is that if, perhaps, we can gain insight into features of %&#10;symmetric and skew-symmetric matrices, then we can apply those understandings %&#10;to other matrices using the decomposition.&#10;\par&#10;\end{document}" display="block">
  <mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>k</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>4</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>f</mi>
      <mi>i</mi>
      <mi>l</mi>
      <mi>l</mi>
      <mo>=</mo>
      <mi>b</mi>
      <mi>l</mi>
      <mi>a</mi>
      <mi>c</mi>
      <mi>k</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mi>n</mi>
    <mi>o</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>3</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\geometry</mtext>
    </merror>
    <mi>l</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>p</mi>
    <mi>a</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>w</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mo>=</mo>
    <mn>17</mn>
    <mi>c</mi>
    <mi>m</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>h</mi>
    <mi>t</mi>
    <mo>=</mo>
    <mn>22</mn>
    <mi>c</mi>
    <mi>m</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>a</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>w</mi>
    <mi>s</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>p</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>k</mi>
    <mi>s</mi>
    <mtext xml:id="Sx1">RecallMatrix Algebra: addition, subtraction, scalar multiplication, and matrix multiplication.Matrix multiplication can be thought of in several ways:ğ€ğ=[â†ithâ†’]â¢[â†‘jthâ†“]ğ€ğ=[â†ithâ†’]â¢[â†‘jthâ†“]=Linear combination of columns ofÂ ğ€=Linear combination of columns ofÂ ğ€=Linear combination of rows ofÂ ğ=Linear combination of rows ofÂ ğFor ğ€ğ to be defined, the number of columns of ğ€ must equal the number of rows of ğ. If ğ€mÃ—n and ğnÃ—p, then their product ğ‚=ğ€ğ is defined andğ‚=(ğ€ğ)mÃ—pğ‚=(ğ€ğ)mÃ—pIt is important to note a few points:1.ğğ€ need not be even defined. Thus ğ€ğâ‰ ğğ€.Consider for exampleğ€3Ã—4â€ƒâ€ƒğ4Ã—5â€ƒâ€ƒ(ğ€ğ)3Ã—5ğ€3Ã—4â€ƒâ€ƒğ4Ã—5â€ƒâ€ƒ(ğ€ğ)3Ã—5But ğğ€ is not defined.2.In some cases, both ğ€ğ and ğğ€ are defined. However, it is still not the case that they are equal. Suppose we have two matrices, ğ€3Ã—2 and ğ2Ã—3. Then the dimensions of their products will be given by(ğ€ğ)3Ã—3â‰ (ğğ€)2Ã—2(ğ€ğ)3Ã—3â‰ (ğğ€)2Ã—2since the dimensions do not match, the matrices cannot be equal.3.Finally, even if ğ€ğ and ğğ€ are of the same size, they still may be unequal. Suppose we have two matrices, ğ€2Ã—2 and ğ2Ã—2. Both of their products will be of size 2Ã—2, yet they may not be equal:(1234)â¢(0712)â‰ (0712)â¢(1234)(1234)â¢(0712)â‰ (0712)â¢(1234)this is most easily determined by comparing the the top-left entry of both products: 2 for the first matrix, and 21 for the second.We can see then that matrix multiplication is fundamentally a non-commutative operation. This is very different from the kind of algebra that we are familiar with. There are several other instances where matrix multiplication leads to counterintuitive results.1.Consider (A+B)2. We normally write (A+B)2=A2+2â¢Aâ¢B+B2 for numbers, but note that for matrices,(ğ€+ğ)2=(ğ€+ğ)â¢(ğ€+ğ)(ğ€+ğ)2=(ğ€+ğ)â¢(ğ€+ğ)=ğ€â¢(ğ€+ğ)+ğâ¢(ğ€+ğ)=ğ€â¢(ğ€+ğ)+ğâ¢(ğ€+ğ)=ğ€ğ€+ğ€ğ+ğğ€+ğğ=ğ€ğ€+ğ€ğ+ğğ€+ğğ=ğ€2+ğ€ğ+ğğ€+ğ2=ğ€2+ğ€ğ+ğğ€+ğ2As we have just found, ğ€ğ need not be equal to ğğ€, so we cannot write that ğ€ğ+ğğ€=2â¢ğ€ğ.2.Consider also the normally valid identityA2-B2=(A-B)â¢(A+B)A2-B2=(A-B)â¢(A+B)This identity need not hold for matrices.3.We regularly make use of the fact that if Aâ¢B=0, then one of A or B (or both) are zero. This is a fundamental property of what are called integral domains. Does this hold for matrices?[1000]â¢[0002]=[0000][1000]â¢[0002]=[0000]The two matrices on the left are non-zero matrices, yet their product is the zero matrix. Thus ğ€ğ=0 does not imply that one of ğ€ or ğ are zeroMatrix EquationsWe have discussed previously, and become comfortable with the notion that a system of linear equations can be expressed as a so-called augmented matrix:x+3â¢y=52â¢x-3â¢y=7}â‡”[[rr|r]1352-37]x+3â¢y=52â¢x-3â¢y=7}â‡”[[rr|r]1352-37]Now however, we can also write this system asx+3â¢y=52â¢x-3â¢y=7}âŸ¹[[râ¢r]â¢132-3][xy]=[57]x+3â¢y=52â¢x-3â¢y=7}âŸ¹[[râ¢r]â¢132-3][xy]=[57]This form should look very similar: ğ€â¢xâ†’=bâ†’. That is, this looks like a regular linear equation.Special Types of MatricesThere are many special types of matrices.Square Matrices: (nÃ—n)1.Diagonal matrix:A diagonal matrix has non-zero entries only on its main diagonal.[d10â‹¯00d2â‹¯0â‹®â‹®â‹±â‹®00â‹¯dn][d10â‹¯00d2â‹¯0â‹®â‹®â‹±â‹®00â‹¯dn]ai,j=0â¢Â ifÂ iâ‰ jai,j=0â¢Â ifÂ iâ‰ j2.Upper triangular matrixAn upper triangular matrix has only zero entries below its main diagonal.[u1,1u1,2â‹¯u1,n0u2,2â‹¯u2,nâ‹®â‹®â‹±â‹®00â‹¯un,n][u1,1u1,2â‹¯u1,n0u2,2â‹¯u2,nâ‹®â‹®â‹±â‹®00â‹¯un,n]ui,j=0â¢Â ifÂ i&lt;jui,j=0â¢Â ifÂ i&lt;j3.Lower triangular matrixA lower triangular matrix has only zero entries above its main diagonal.[l1,10â‹¯0l2,1l2,2â‹¯0â‹®â‹®â‹±â‹®ln,1ln,2â‹¯ln,n][l1,10â‹¯0l2,1l2,2â‹¯0â‹®â‹®â‹±â‹®ln,1ln,2â‹¯ln,n]ui,j=0â¢Â ifÂ i&gt;jui,j=0â¢Â ifÂ i&gt;j4.Identity matrix (ğˆn)The identity matrix is a diagonal matrix with 1â€™s on all of its diagonal entries.ğˆn=[10â‹¯001â‹¯0â‹®â‹®â‹±â‹®00â‹¯1]ğˆn=[10â‹¯001â‹¯0â‹®â‹®â‹±â‹®00â‹¯1]The identity matrix is very important because multiplying any square nÃ—n matrix ğ€ by the identity matrix ğˆn will yield ğ€:ğ€ğˆn=ğˆnâ¢ğ€=ğ€ğ€ğˆn=ğˆnâ¢ğ€=ğ€In this way, ğˆn in nÃ—n matrices is like the number ğŸ in the real numbers.5.Permutation matrixA permutation matrix is formed by permuting (swapping) the rows of the identity matrix.[001100010]â¢[xyz]=[zxy][001100010]â¢[xyz]=[zxy]Remark: The set of all permutation matrices of a given size n is called Sn. For example:S3={ğ€3Ã—3|ğ€Â is a permutation matrix}S3={ğ€3Ã—3|ğ€Â is a permutation matrix}Note that taking any two elements in S3 and multiplying them together yields another element in S3. That is, S3 is closed under multiplication. The name for this type of structure is a group. One feature is that |S3|=3!=6. There are n! permutation matrices of a given size n.Transpose: ğ€TTransposition is an operation unique to matrices. Let ğ€mÃ—n be a matrix (not necessarily square). Then we define the transpose of ğ€, written as ğ€T, by letting its columns be the rows of ğ€.ğ€mÃ—nâ‡’ğ€nÃ—mTğ€mÃ—nâ‡’ğ€nÃ—mTFor example,ğ€=[123456]ğ€=[123456]ğ€T=[142536]ğ€T=[142536]Some properties of the transpose include:1.(ğ€+ğ)T=ğ€T+ğT2.(kâ¢ğ€)T=kâ¢ğ€T3.(ğ€ğ)T=ğTâ¢ğ€T4.(ğ€T)T=ğ€Symmetric MatricesTransposition allows us to define two important types of matrices:â€¢ğ€ is symmetric if ğ€T=ğ€.â€¢ğ€ is skew-symmetric if ğ€T=-ğ€.Examining these definitions, it is easy to see that a symmetric or skew-symmetric matrix must be an nÃ—n matrix, even though transposition is defined for all matrices.Let us examine some features of symmetric matrices. Consider ğ=ğ€+ğ€T with ğ€ an nÃ—n matrix. Is ğ symmetric?ğ=(ğ€+ğ€T)Tğ=(ğ€+ğ€T)T=ğ€T+(ğ€T)T=ğ€T+(ğ€T)T=ğ€T+ğ€=ğ€T+ğ€=ğ=ğTherefore ğ is symmetric.Consider ğ‚=ğ€-ğ€T. Is ğ‚ symmetric / skew-symmetric?ğ‚T=(ğ€-ğ€T)Tğ‚T=(ğ€-ğ€T)T=ğ€T-(ğ€T)T=ğ€T-(ğ€T)T=ğ€T-ğ€=ğ€T-ğ€=-ğ‚=-ğ‚Thus ğ‚ is skew-symmetric.It is natural to wonder what types of matrices are symmetric or skew-symmetric. Consider for example:ğ€=[1234]ğ€=[1234]ğ€T=[1324]ğ€T=[1324]Note that ğ€Tâ‰ ğ€ and ğ€Tâ‰ -ğ€. Thus ğ€ is neither symmetric nor skew-symmetric.An important observation can be made by asking: what kinds of matrices can in general be skew-symmetric?ğ€=[abcd]ğ€=[abcd]ğ€T=[acbd]ğ€T=[acbd]For ğ€ to be skew-symmetric,ğ€T=-ğ€ğ€T=-ğ€[acbd]=[-a-b-c-d][acbd]=[-a-b-c-d]Thus a=-a and d=-d. This can only be true when a=d=0. This can be generalized to any size n:Proposition: If ğ€ is a skew-symmetric matrix then the diagonal entries must be 0.Here is a question to consider. If you recall, for any function fâ¢(x), it is possible to write fâ¢(x) as the sum of an even and an odd function. Is a similar result true for matrices?fâ¢(x)=even function+odd functionfâ¢(x)=even function+odd functionğ€nÃ—n=symmetric matrix+skew-symmetric matrixğ€nÃ—n=symmetric matrix+skew-symmetric matrixIf it were true that this is the case, then we would wantğ€=ğ+ğ‚ğ€=ğ+ğ‚where ğ is symmetric and ğ‚ is skew-symmetric. Thenğ€T=ğT+ğ‚T=ğ-ğ‚ğ€T=ğT+ğ‚T=ğ-ğ‚Adding and subtracting these two equations, we obtain the two equationsğ€+ğ€T2=ğğ€+ğ€T2=ğğ€-ğ€T2=ğ‚ğ€-ğ€T2=ğ‚What is the use in this result? Apart from it being interesting that this type of expression of an arbitrary matrix is always possible, the advantage of this decomposition is that if, perhaps, we can gain insight into features of symmetric and skew-symmetric matrices, then we can apply those understandings to other matrices using the decomposition.</mtext>
  </mrow>
</math>
