<?xml version="1.0" encoding="UTF-8"?>
<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\documentclass[11pt]{article}\usepackage{aahomework}\usepackage{mathtools}%&#10;\usepackage{subcaption}\usepackage{epstopdf}\usepackage{float}\usepackage{%&#10;xcolor}\usepackage{parskip}\tikzstyle{blk}=[circle,innersep=0pt,minimumsize=4%&#10;pt,draw,fill=black,linewidth=0.8pt]\tikzstyle{blanknode}=[circle,innersep=3pt,%&#10;minimumsize=8pt,draw,linewidth=0.8pt]\par&#10;\geometry{letterpaper,textwidth=17cm%&#10;,textheight=22cm}\par&#10;\usetikzlibrary{arrows}\usetikzlibrary{plotmarks}\par&#10;%&#10;\par&#10;\par&#10;\par&#10;\begin{document}&#10;\par&#10;\@@section{section}{Sx1}{}{}{}{Recall}&#10;\par&#10; Previously, we saw the inverse $\mathbf{M}$ of a matrix $\mathbf{A}$:&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{M}=\mathbf{M}\mathbf{A}=\mathbf{I}_%&#10;{n}$&#10;where $\mathbf{A}$ is an $n\times n$ matrix; that is, $\mathbf{A}$ must be %&#10;square to have an inverse.&#10;\par&#10; In a $3\times 3$ matrix, this equates to solving each of&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix}1\\&#10;0\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;1\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;0\\&#10;1\end{bmatrix}$&#10;which can be formulated as the process of transforming an extended augmented %&#10;matrix through row reductions:&#10;\@@amsalign$\displaystyle\left[\begin{matrix}[c|c]\mathbf{A}&amp;\mathbf{I}\end{%&#10;matrix}\right]\longrightarrow\left[\begin{matrix}[c|c]\mathbf{I}&amp;\mathbf{M}%&#10;\end{matrix}\right]$&#10;\par&#10;\@@section{section}{Sx2}{}{}{}{Fundamental Theorem of Invertible Matrices%&#10;}&#10;\par&#10;\framebox[][r]{&#10;\begin{minipage}{0.0pt}&#10;Let $\mathbf{A}_{n\times n}$ be a matrix over a field $\mathbb{F}$. Then the %&#10;following are equivalent (TFAE):&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$\mathbf{A}$ is invertible.&#10;}&#10;\enumerate@item{&#10;The reduced row echelon form (rref) of $\mathbf{A}$ is $\mathbf{I}$.&#10;}&#10;\enumerate@item{&#10;The $\text{rank}(A)=n$.&#10;}&#10;\enumerate@item{&#10;The columns of $\mathbf{A}$ are linearly independent.&#10;}&#10;\enumerate@item{&#10;The rows of $\mathbf{A}$ are linearly independent.&#10;}&#10;\enumerate@item{&#10;The \emph{homogeneous system} $\mathbf{A}\vec{x}=\vec{0}$ has only the zero %&#10;solution $\vec{x}=\vec{0}$.&#10;}&#10;\enumerate@item{&#10;$\mathbf{A}\vec{x}=\vec{b}$ will have a unique solution for all $\vec{b}\in%&#10;\bbR^{n}$.&#10;}&#10;\enumerate@item{&#10;The span of the columns of $\mathbf{A}=\bbR^{n}$.&#10;}&#10;\enumerate@item{&#10;The columns and rows of $\mathbf{A}$ form a basis of $\bbR^{n}$.&#10;}&#10;\enumerate@item{&#10;The $\det(\mathbf{A})\neq 0$.&#10;}&#10;\enumerate@item{&#10;$\mathbf{A}$ is the product of elementary matrices.&#10;}&#10;\end{enumerate}&#10;\end{minipage}&#10;}&#10;\par&#10;\@@section{paragraph}{Sx2.SS0.SSS0.Px1}{}{}{}{Proof of (6).} Consider $%&#10;\mathbf{A}\vec{x}=\vec{0}$. If $\mathbf{A}$ has an inverse $\mathbf{M}$, then&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\vec{0}$\\&#10;$\displaystyle\mathbf{M}\mathbf{A}\vec{x}$&amp;$\displaystyle=\mathbf{M}\vec{0}$\\&#10;$\displaystyle\mathbf{I}\vec{x}$&amp;$\displaystyle=\vec{0}$\\&#10;$\displaystyle\vec{x}$&amp;$\displaystyle=\vec{0}$&#10;\par&#10;\@@section{paragraph}{Sx2.SS0.SSS0.Px2}{}{}{}{Proof of (7).} Suppose $%&#10;\vec{b}\in\bbR^{n}$ is an arbitrary vector. Next consider the system&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\vec{b}$\\&#10;$\displaystyle\mathbf{M}\mathbf{A}\vec{x}$&amp;$\displaystyle=\mathbf{M}\vec{b}$\\&#10;$\displaystyle\vec{x}$&amp;$\displaystyle=\mathbf{M}\vec{b}$&#10;so the system has a unique solution for any $\vec{b}$.&#10;\par&#10;\vspace{1cm}&#10;\par&#10; Let us take a closer look at some of the implications of this theorem. %&#10;Suppose $\mathbf{A}$ is a $3\times 3$ matrix. Suppose each of the following %&#10;systems&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix}1\\&#10;0\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;1\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;0\\&#10;1\end{bmatrix}$&#10;has a solution, called $\vec{x}_{1},\vec{x}_{2},\vec{x}_{3}$, respectively.&#10;\par&#10; Suppose we want to find the solution of $\mathbf{A}\vec{x}=\begin{%&#10;bmatrix}1\\&#10;e\\&#10;\pi\end{bmatrix}$. Note that&#10;\@@amsalign$\displaystyle\begin{bmatrix}1\\&#10;e\\&#10;\pi\end{bmatrix}$&amp;$\displaystyle=1\begin{bmatrix}1\\&#10;0\\&#10;0\end{bmatrix}+e\begin{bmatrix}0\\&#10;1\\&#10;0\end{bmatrix}+\pi\begin{bmatrix}0\\&#10;0\\&#10;1\end{bmatrix}$\\&#10;$$&amp;$\displaystyle=1\mathbf{A}\vec{x}_{1}+e\mathbf{A}\vec{x}_{2}+\pi\mathbf{A}%&#10;\vec{x}_{3}$\\&#10;$$&amp;$\displaystyle=A\Big(1\vec{x}_{1}+e\vec{x}_{2}+\pi\vec{x}_{3}\Big)$&#10;So the solution is given by $\vec{x}_{1}+e\vec{x}_{2}+\pi\vec{x}_{3}$.&#10;\par&#10;\@@section{section}{Sx3}{}{}{}{Elementary Matrices}&#10;\par&#10; An {elementary matrix}, $\mathbf{E}$, is a matrix we obtain by %&#10;performing exactly one row operation on an identity matrix.&#10;\par&#10; For example, in the $2\times 2$ case,&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{2R_{2}}\begin{bmatrix}1&amp;0\\&#10;0&amp;2\end{bmatrix}$&amp;$$&amp;$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{-3R_{1}+R_{2}}\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}$&amp;$$&amp;$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{R_{2}\circlearrowright R_{1}}\begin{bmatrix}0&amp;1%&#10;\\&#10;1&amp;0\end{bmatrix}$&#10;The matrices $\begin{bmatrix}1&amp;0\\&#10;0&amp;2\end{bmatrix},\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix},\begin{bmatrix}0&amp;1\\&#10;1&amp;0\end{bmatrix}$ are all elementary matrices.&#10;\par&#10; Consider multiplying a $2\times 2$ matrix $\mathbf{A}$ by the second of %&#10;these elementary matrices:&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}\begin{bmatrix}a&amp;b\\&#10;c&amp;d\end{bmatrix}$\\&#10;$$&amp;$\displaystyle=\begin{bmatrix}a&amp;b\\&#10;-3a+c&amp;-3b+d\end{bmatrix}$\\&#10;$$\\&#10;$\displaystyle\begin{bmatrix}a&amp;b\\&#10;c&amp;d\end{bmatrix}\xrightarrow[]{-3R_{1}+R_{2}}$&amp;$\displaystyle\begin{bmatrix}a&amp;%&#10;b\\&#10;-3a+c&amp;-3b+d\end{bmatrix}$&#10;We find that multiplying $\mathbf{A}$ by an elementary matrix has the same %&#10;effect as performing the same single row operation on $\mathbf{A}$ as was %&#10;performed to obtain the elementary matrix. This shows us that to perform one %&#10;row operation on a matrix $\mathbf{A}$ we just need to multiply $\mathbf{A}$ %&#10;on the left by an appropriate elementary matrix.&#10;\par&#10;\@@section{subsection}{Sx3.SSx1}{}{}{}{Properties of Inverses}&#10;For a matrix $\mathbf{A}$, the following are true of its inverse $\mathbf{A}^{%&#10;-1}$:&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$\left(\mathbf{A}^{-1}\right)^{-1}=\mathbf{A}$&#10;}&#10;\enumerate@item{&#10;$\left(k\mathbf{A}\right)^{-1}=\frac{1}{k}\mathbf{A}^{-1}$&#10;}&#10;\enumerate@item{&#10;$\mathbf{A}^{-1}$ is unique&#10;}&#10;\enumerate@item{&#10;$\left(\mathbf{A}\mathbf{B}\right)^{-1}=\mathbf{B}^{-1}\mathbf{A}^{-1}$&#10;}&#10;\end{enumerate}&#10;\par&#10;\proof&#10;Suppose $\mathbf{M}_{1}$ and $\mathbf{M}_{2}$ act as inverses of $\mathbf{A}$.%&#10; That is,&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{M}_{1}$&amp;$\displaystyle=\mathbf{M}_{%&#10;1}\mathbf{A}$&amp;$\displaystyle=\mathbf{I}$\\&#10;$\displaystyle\mathbf{A}\mathbf{M}_{2}$&amp;$\displaystyle=\mathbf{M}_{2}\mathbf{A%&#10;}$&amp;$\displaystyle=\mathbf{I}$&#10;Note then that&#10;\@@amsalign$\displaystyle\mathbf{M}_{1}$&amp;$\displaystyle=\mathbf{M}_{1}\mathbf{%&#10;I}$\\&#10;$$&amp;$\displaystyle=\mathbf{M}_{1}\left(\mathbf{A}\mathbf{M}_{2}\right)$\\&#10;$$&amp;$\displaystyle=\left(\mathbf{M}_{1}\mathbf{A}\right)\mathbf{M}_{2}$\\&#10;$$&amp;$\displaystyle=\mathbf{I}\mathbf{M}_{2}$&#10;So $\mathbf{M}_{1}=\mathbf{M}_{2}$.&#10;\par&#10;\proof&#10;Assume that $\mathbf{A}$ and $\mathbf{B}$ are both invertible.&#10;\@@amsalign$\displaystyle\left(\mathbf{A}\mathbf{B}\right)\mathbf{B}^{-1}%&#10;\mathbf{A}^{-1}$&amp;$\displaystyle=\mathbf{A}\left(\mathbf{B}\mathbf{B}^{-1}%&#10;\right)\mathbf{A}^{-1}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\mathbf{I}\mathbf{A}^{-1}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\mathbf{A}^{-1}$\\&#10;$$&amp;$\displaystyle=I$&#10;Similarly we can show that $\mathbf{B}^{-1}\mathbf{A}^{-1}\left(\mathbf{A}%&#10;\mathbf{B}\right)=\mathbf{I}$.&#10;\par&#10;\@@section{paragraph}{Sx3.SSx1.SSS0.Px1}{}{}{}{Question:}Can $\left(%&#10;\mathbf{A}\mathbf{B}\right)$ be invertible while one of $\mathbf{A}$ or $%&#10;\mathbf{B}$ is not invertible? Must one be invertible, must both be invertible%&#10;?&#10;\par&#10; What if $\mathbf{B}$ is not invertible? By the fundamental theorem of %&#10;invertible matrices, we have that $\mathbf{B}\vec{x}=\vec{0}$ must have a non-%&#10;trivial solution, $\vec{x}_{0}$. Then&#10;\@@amsalign$\displaystyle\left(\mathbf{A}\mathbf{B}\right)\vec{x}_{0}$&amp;$%&#10;\displaystyle=\mathbf{A}\left(\mathbf{B}\vec{x}_{0}\right)$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\vec{0}$\\&#10;$$&amp;$\displaystyle=\vec{0}$&#10;Then the equation $\left(\mathbf{A}\mathbf{b}\right)\vec{x}=\vec{0}$ also has %&#10;a non-trivial solution. Thus $\left(\mathbf{A}\mathbf{B}\right)$ cannot be %&#10;invertible.&#10;\par&#10;\@@section{section}{Sx4}{}{}{}{Elementary Matrices and Invertible %&#10;Matrices}&#10;Consider the following matrix $\mathbf{A}$ and its row reduced echelon form:&#10;\@@amsalign$\displaystyle\mathbf{A}=\begin{bmatrix}1&amp;2\\&#10;3&amp;4\end{bmatrix}\xrightarrow[]{-3R_{1}+R_{2}}\begin{bmatrix}1&amp;2\\&#10;0&amp;-2\end{bmatrix}\xrightarrow[]{\frac{-1}{2}R_{2}}\begin{bmatrix}1&amp;2\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{-2R_{1}+R_{2}}\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}$&#10;By the fundamental theorem of matrices, this shows us that $\mathbf{A}$ is %&#10;invertible.&#10;\par&#10; Note though that we have said that we can represent row operations as %&#10;elementary matrices. Corresponding to these row operations then, are the %&#10;matrices&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;2\\&#10;3&amp;4\end{bmatrix}\underbrace{\xrightarrow[]{-3R_{1}+R_{2}}}_{\mathbf{E}=\begin{%&#10;bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}}\begin{bmatrix}1&amp;2\\&#10;0&amp;-2\end{bmatrix}\underbrace{\xrightarrow[]{\frac{-1}{2}R_{2}}}_{\mathbf{F}=%&#10;\begin{bmatrix}1&amp;0\\&#10;0&amp;\frac{-1}{2}\end{bmatrix}}\begin{bmatrix}1&amp;2\\&#10;0&amp;1\end{bmatrix}\underbrace{\xrightarrow[]{-2R_{1}+R_{2}}}_{\mathbf{G}=\begin{%&#10;bmatrix}1&amp;-2\\&#10;0&amp;1\end{bmatrix}}\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}$&#10;If we call the matrices between $\mathbf{A}$ and $\mathbf{I}$ as $\mathbf{B}$ %&#10;and $\mathbf{C}$, then we have that&#10;\@@amsalign$\displaystyle\mathbf{B}$&amp;$\displaystyle=\mathbf{E}\mathbf{A}$\\&#10;$\displaystyle\mathbf{C}$&amp;$\displaystyle=\mathbf{F}\mathbf{B}=\mathbf{F}%&#10;\mathbf{E}\mathbf{A}$\\&#10;$\displaystyle\mathbf{I}$&amp;$\displaystyle=\mathbf{G}\mathbf{C}=\mathbf{G}%&#10;\mathbf{F}\mathbf{E}\mathbf{A}$&#10;and so $\left(\mathbf{G}\mathbf{F}\mathbf{E}\right)$ is the inverse of $%&#10;\mathbf{A}$:&#10;\@@amsalign$\displaystyle\left(\mathbf{G}\mathbf{F}\mathbf{E}\right)\mathbf{A}%&#10;$&amp;$\displaystyle=\mathbf{I}$&#10;so&#10;\@@amsalign$\displaystyle\mathbf{A}=\left(\mathbf{G}\mathbf{F}\mathbf{E}\right%&#10;)^{-1}=\mathbf{E}^{-1}\mathbf{F}^{-1}\mathbf{G}^{-1}$&#10;but this assumes that the matrices $\mathbf{E},\mathbf{F},\mathbf{G}$ are %&#10;invertible. This brings us to an important question:&#10;\@@section{paragraph}{Sx4.SSx1.SSS0.Px1}{}{}{}{Question:}Are elementary %&#10;matrices invertible?&#10;In essence, this equates to asking how to reverse a row operation. Every row %&#10;operation can be reversed, and so every elementary matrix is invertible.&#10;\@@amsalign$\displaystyle\mathbf{E}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}$&amp;$\displaystyle\mathbf{F}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0%&#10;\\&#10;0&amp;\frac{-1}{2}\end{bmatrix}$&amp;$\displaystyle\mathbf{G}$&amp;$\displaystyle=\begin{%&#10;bmatrix}1&amp;-2\\&#10;0&amp;1\end{bmatrix}$\\&#10;$\displaystyle\mathbf{E}^{-1}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0\\&#10;3&amp;0\end{bmatrix}$&amp;$\displaystyle\mathbf{F}^{-1}$&amp;$\displaystyle=\begin{bmatrix%&#10;}1&amp;0\\&#10;0&amp;-2\end{bmatrix}$&amp;$\displaystyle\mathbf{G}^{-1}$&amp;$\displaystyle=\begin{%&#10;bmatrix}1&amp;2\\&#10;0&amp;1\end{bmatrix}$&#10;\par&#10;\@@section{section}{Sx5}{}{}{}{Vector Space}&#10;The term vector space shouldn't be confused for what we know as vectors. %&#10;Consider the set&#10;\@@amsalign$\displaystyle\mathbb{P}_{2}$&amp;$\displaystyle=\text{Polynomials of %&#10;degree $\leq 2$}$&#10;That is, elements of $\mathbb{P}_{2}$ are of the form&#10;\@@amsalign$\displaystyle ax^{2}+bx+c$&#10;where $a,b,c\in\bbR$.&#10;\par&#10; Let $u,v\in\mathbb{P}_{2}$. It is apparent then that $u(x)+v(x)\in%&#10;\mathbb{P}_{2}$ and for any $k\in\bbR$, $ku(x)\in\mathbb{P}_{2}$.&#10;\par&#10; This is very similar to $\bbR^{3}$. Consider $\vec{u},\vec{v}\in\bbR^{3}%&#10;$. Then $\vec{u}+\vec{v}\in\bbR^{3}$ and for any $k\in\bbR^{3}$, $k\vec{u}\in%&#10;\bbR^{3}$.&#10;\par&#10; In fact, many things besides standard vectors will be considered as %&#10;vectors. Polynomials, functions, and matrices can all be seen as vectors in %&#10;their respective vector spaces.&#10;\par&#10;\par&#10;\end{document}" display="block">
  <mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>k</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>4</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>f</mi>
      <mi>i</mi>
      <mi>l</mi>
      <mi>l</mi>
      <mo>=</mo>
      <mi>b</mi>
      <mi>l</mi>
      <mi>a</mi>
      <mi>c</mi>
      <mi>k</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mi>n</mi>
    <mi>o</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>3</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\geometry</mtext>
    </merror>
    <mi>l</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>p</mi>
    <mi>a</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>w</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mo>=</mo>
    <mn>17</mn>
    <mi>c</mi>
    <mi>m</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>h</mi>
    <mi>t</mi>
    <mo>=</mo>
    <mn>22</mn>
    <mi>c</mi>
    <mi>m</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>a</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>w</mi>
    <mi>s</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>p</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>k</mi>
    <mi>s</mi>
    <mtext xml:id="Sx1">RecallPreviously, we saw the inverse ğŒ of a matrix ğ€:ğ€ğŒ=ğŒğ€=ğˆnğ€ğŒ=ğŒğ€=ğˆnwhere ğ€ is an nÃ—n matrix; that is, ğ€ must be square to have an inverse.In a 3Ã—3 matrix, this equates to solving each ofğ€â¢xâ†’=[100]ğ€â¢xâ†’=[100]ğ€â¢xâ†’=[010]ğ€â¢xâ†’=[010]ğ€â¢xâ†’=[001]ğ€â¢xâ†’=[001]which can be formulated as the process of transforming an extended augmented matrix through row reductions:[[c|c]ğ€ğˆ]âŸ¶[[c|c]ğˆğŒ][[c|c]ğ€ğˆ]âŸ¶[[c|c]ğˆğŒ]Fundamental Theorem of Invertible Matrices
Let ğ€nÃ—n be a matrix over a field ğ”½. Then the following are equivalent (TFAE):1.ğ€ is invertible.2.The reduced row echelon form (rref) of ğ€ is ğˆ.3.The rankâ¢(A)=n.4.The columns of ğ€ are linearly independent.5.The rows of ğ€ are linearly independent.6.The homogeneous system ğ€â¢xâ†’=0â†’ has only the zero solution xâ†’=0â†’.7.ğ€â¢xâ†’=bâ†’ will have a unique solution for all bâ†’âˆˆ\bbRn.8.The span of the columns of ğ€=\bbRn.9.The columns and rows of ğ€ form a basis of \bbRn.10.The detâ¡(ğ€)â‰ 0.11.ğ€ is the product of elementary matrices.
Proof of (6).Consider ğ€â¢xâ†’=0â†’. If ğ€ has an inverse ğŒ, thenğ€â¢xâ†’=0â†’ğ€â¢xâ†’=0â†’ğŒğ€â¢xâ†’=ğŒâ¢0â†’ğŒğ€â¢xâ†’=ğŒâ¢0â†’ğˆâ¢xâ†’=0â†’ğˆâ¢xâ†’=0â†’xâ†’=0â†’xâ†’=0â†’Proof of (7).Suppose bâ†’âˆˆ\bbRn is an arbitrary vector. Next consider the systemğ€â¢xâ†’=bâ†’ğ€â¢xâ†’=bâ†’ğŒğ€â¢xâ†’=ğŒâ¢bâ†’ğŒğ€â¢xâ†’=ğŒâ¢bâ†’xâ†’=ğŒâ¢bâ†’xâ†’=ğŒâ¢bâ†’so the system has a unique solution for any bâ†’.Let us take a closer look at some of the implications of this theorem. Suppose ğ€ is a 3Ã—3 matrix. Suppose each of the following systemsğ€â¢xâ†’=[100]ğ€â¢xâ†’=[100]ğ€â¢xâ†’=[010]ğ€â¢xâ†’=[010]ğ€â¢xâ†’=[001]ğ€â¢xâ†’=[001]has a solution, called xâ†’1,xâ†’2,xâ†’3, respectively.Suppose we want to find the solution of ğ€â¢xâ†’=[1eÏ€]. Note that[1eÏ€]=1â¢[100]+eâ¢[010]+Ï€â¢[001][1eÏ€]=1â¢[100]+eâ¢[010]+Ï€â¢[001]=1â¢ğ€â¢xâ†’1+eâ¢ğ€â¢xâ†’2+Ï€â¢ğ€â¢xâ†’3=1â¢ğ€â¢xâ†’1+eâ¢ğ€â¢xâ†’2+Ï€â¢ğ€â¢xâ†’3=Aâ¢(1â¢xâ†’1+eâ¢xâ†’2+Ï€â¢xâ†’3)=Aâ¢(1â¢xâ†’1+eâ¢xâ†’2+Ï€â¢xâ†’3)So the solution is given by xâ†’1+eâ¢xâ†’2+Ï€â¢xâ†’3.Elementary MatricesAn elementary matrix, ğ„, is a matrix we obtain by performing exactly one row operation on an identity matrix.For example, in the 2Ã—2 case,[1001]â†’2â¢R2[1002][1001]â†’2â¢R2[1002][1001]â†’-3â¢R1+R2[10-31][1001]â†’-3â¢R1+R2[10-31][1001]â†’R2â¢\circlearrowrightâ¢R1[0110][1001]â†’R2â¢\circlearrowrightâ¢R1[0110]The matrices [1002],[10-31],[0110] are all elementary matrices.Consider multiplying a 2Ã—2 matrix ğ€ by the second of these elementary matrices:[10-31]â¢ğ€=[10-31]â¢[abcd][10-31]â¢ğ€=[10-31]â¢[abcd]=[ab-3â¢a+c-3â¢b+d]=[ab-3â¢a+c-3â¢b+d][abcd]â†’-3â¢R1+R2[ab-3â¢a+c-3â¢b+d][abcd]â†’-3â¢R1+R2[ab-3â¢a+c-3â¢b+d]We find that multiplying ğ€ by an elementary matrix has the same effect as performing the same single row operation on ğ€ as was performed to obtain the elementary matrix. This shows us that to perform one row operation on a matrix ğ€ we just need to multiply ğ€ on the left by an appropriate elementary matrix.Properties of InversesFor a matrix ğ€, the following are true of its inverse ğ€-1:1.(ğ€-1)-1=ğ€2.(kâ¢ğ€)-1=1kâ¢ğ€-13.ğ€-1 is unique4.(ğ€ğ)-1=ğ-1â¢ğ€-1{proof}Suppose ğŒ1 and ğŒ2 act as inverses of ğ€. That is,ğ€ğŒ1=ğŒ1â¢ğ€ğ€ğŒ1=ğŒ1â¢ğ€=ğˆ=ğˆğ€ğŒ2=ğŒ2â¢ğ€ğ€ğŒ2=ğŒ2â¢ğ€=ğˆ=ğˆNote then thatğŒ1=ğŒ1â¢ğˆğŒ1=ğŒ1â¢ğˆ=ğŒ1â¢(ğ€ğŒ2)=ğŒ1â¢(ğ€ğŒ2)=(ğŒ1â¢ğ€)â¢ğŒ2=(ğŒ1â¢ğ€)â¢ğŒ2=ğˆğŒ2=ğˆğŒ2So ğŒ1=ğŒ2.{proof}Assume that ğ€ and ğ are both invertible.(ğ€ğ)â¢ğ-1â¢ğ€-1=ğ€â¢(ğğ-1)â¢ğ€-1(ğ€ğ)â¢ğ-1â¢ğ€-1=ğ€â¢(ğğ-1)â¢ğ€-1=ğ€ğˆğ€-1=ğ€ğˆğ€-1=ğ€ğ€-1=ğ€ğ€-1=I=ISimilarly we can show that ğ-1â¢ğ€-1â¢(ğ€ğ)=ğˆ.Question:Can (ğ€ğ) be invertible while one of ğ€ or ğ is not invertible? Must one be invertible, must both be invertible?What if ğ is not invertible? By the fundamental theorem of invertible matrices, we have that ğâ¢xâ†’=0â†’ must have a non-trivial solution, xâ†’0. Then(ğ€ğ)â¢xâ†’0=ğ€â¢(ğâ¢xâ†’0)(ğ€ğ)â¢xâ†’0=ğ€â¢(ğâ¢xâ†’0)=ğ€â¢0â†’=ğ€â¢0â†’=0â†’=0â†’Then the equation (ğ€ğ›)â¢xâ†’=0â†’ also has a non-trivial solution. Thus (ğ€ğ) cannot be invertible.Elementary Matrices and Invertible MatricesConsider the following matrix ğ€ and its row reduced echelon form:ğ€=[1234]â†’-3â¢R1+R2[120-2]â†’-12â¢R2[1201]â†’-2â¢R1+R2[1001]ğ€=[1234]â†’-3â¢R1+R2[120-2]â†’-12â¢R2[1201]â†’-2â¢R1+R2[1001]By the fundamental theorem of matrices, this shows us that ğ€ is invertible.Note though that we have said that we can represent row operations as elementary matrices. Corresponding to these row operations then, are the matrices[1234]â¢â†’-3â¢R1+R2âŸğ„=[10-31]â¢[120-2]â¢â†’-12â¢R2âŸğ…=[100-12]â¢[1201]â¢â†’-2â¢R1+R2âŸğ†=[1-201]â¢[1001][1234]â¢â†’-3â¢R1+R2âŸğ„=[10-31]â¢[120-2]â¢â†’-12â¢R2âŸğ…=[100-12]â¢[1201]â¢â†’-2â¢R1+R2âŸğ†=[1-201]â¢[1001]If we call the matrices between ğ€ and ğˆ as ğ and ğ‚, then we have thatğ=ğ„ğ€ğ=ğ„ğ€ğ‚=ğ…ğ=ğ…ğ„ğ€ğ‚=ğ…ğ=ğ…ğ„ğ€ğˆ=ğ†ğ‚=ğ†ğ…ğ„ğ€ğˆ=ğ†ğ‚=ğ†ğ…ğ„ğ€and so (ğ†ğ…ğ„) is the inverse of ğ€:(ğ†ğ…ğ„)â¢ğ€=ğˆ(ğ†ğ…ğ„)â¢ğ€=ğˆsoğ€=(ğ†ğ…ğ„)-1=ğ„-1â¢ğ…-1â¢ğ†-1ğ€=(ğ†ğ…ğ„)-1=ğ„-1â¢ğ…-1â¢ğ†-1but this assumes that the matrices ğ„,ğ…,ğ† are invertible. This brings us to an important question:Question:Are elementary matrices invertible?
In essence, this equates to asking how to reverse a row operation. Every row operation can be reversed, and so every elementary matrix is invertible.ğ„=[10-31]ğ„=[10-31]ğ…=[100-12]ğ…=[100-12]ğ†=[1-201]ğ†=[1-201]ğ„-1=[1030]ğ„-1=[1030]ğ…-1=[100-2]ğ…-1=[100-2]ğ†-1=[1201]ğ†-1=[1201]Vector SpaceThe term vector space shouldnâ€™t be confused for what we know as vectors. Consider the setâ„™2=Polynomials of degreeÂ â‰¤2â„™2=Polynomials of degreeÂ â‰¤2That is, elements of â„™2 are of the formaâ¢x2+bâ¢x+caâ¢x2+bâ¢x+cwhere a,b,câˆˆ\bbR.Let u,vâˆˆâ„™2. It is apparent then that uâ¢(x)+vâ¢(x)âˆˆâ„™2 and for any kâˆˆ\bbR, kâ¢uâ¢(x)âˆˆâ„™2.This is very similar to \bbR3. Consider uâ†’,vâ†’âˆˆ\bbR3. Then uâ†’+vâ†’âˆˆ\bbR3 and for any kâˆˆ\bbR3, kâ¢uâ†’âˆˆ\bbR3.In fact, many things besides standard vectors will be considered as vectors. Polynomials, functions, and matrices can all be seen as vectors in their respective vector spaces.</mtext>
  </mrow>
</math>
