<?xml version="1.0" encoding="UTF-8"?>
<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\documentclass[11pt]{article}\usepackage{aahomework}\usepackage{mathtools}%&#10;\usepackage{subcaption}\usepackage{epstopdf}\usepackage{float}\usepackage{%&#10;xcolor}\usepackage{parskip}\tikzstyle{blk}=[circle,innersep=0pt,minimumsize=4%&#10;pt,draw,fill=black,linewidth=0.8pt]\tikzstyle{blanknode}=[circle,innersep=3pt,%&#10;minimumsize=8pt,draw,linewidth=0.8pt]\par&#10;\geometry{letterpaper,textwidth=17cm%&#10;,textheight=22cm}\par&#10;\usetikzlibrary{arrows}\usetikzlibrary{plotmarks}\par&#10;%&#10;\par&#10;\par&#10;\par&#10;\begin{document}&#10;\par&#10;\@@section{section}{Sx1}{}{}{}{Recall}&#10;\par&#10; Previously, we saw the inverse $\mathbf{M}$ of a matrix $\mathbf{A}$:&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{M}=\mathbf{M}\mathbf{A}=\mathbf{I}_%&#10;{n}$&#10;where $\mathbf{A}$ is an $n\times n$ matrix; that is, $\mathbf{A}$ must be %&#10;square to have an inverse.&#10;\par&#10; In a $3\times 3$ matrix, this equates to solving each of&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix}1\\&#10;0\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;1\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;0\\&#10;1\end{bmatrix}$&#10;which can be formulated as the process of transforming an extended augmented %&#10;matrix through row reductions:&#10;\@@amsalign$\displaystyle\left[\begin{matrix}[c|c]\mathbf{A}&amp;\mathbf{I}\end{%&#10;matrix}\right]\longrightarrow\left[\begin{matrix}[c|c]\mathbf{I}&amp;\mathbf{M}%&#10;\end{matrix}\right]$&#10;\par&#10;\@@section{section}{Sx2}{}{}{}{Fundamental Theorem of Invertible Matrices%&#10;}&#10;\par&#10;\framebox[][r]{&#10;\begin{minipage}{0.0pt}&#10;Let $\mathbf{A}_{n\times n}$ be a matrix over a field $\mathbb{F}$. Then the %&#10;following are equivalent (TFAE):&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$\mathbf{A}$ is invertible.&#10;}&#10;\enumerate@item{&#10;The reduced row echelon form (rref) of $\mathbf{A}$ is $\mathbf{I}$.&#10;}&#10;\enumerate@item{&#10;The $\text{rank}(A)=n$.&#10;}&#10;\enumerate@item{&#10;The columns of $\mathbf{A}$ are linearly independent.&#10;}&#10;\enumerate@item{&#10;The rows of $\mathbf{A}$ are linearly independent.&#10;}&#10;\enumerate@item{&#10;The \emph{homogeneous system} $\mathbf{A}\vec{x}=\vec{0}$ has only the zero %&#10;solution $\vec{x}=\vec{0}$.&#10;}&#10;\enumerate@item{&#10;$\mathbf{A}\vec{x}=\vec{b}$ will have a unique solution for all $\vec{b}\in%&#10;\bbR^{n}$.&#10;}&#10;\enumerate@item{&#10;The span of the columns of $\mathbf{A}=\bbR^{n}$.&#10;}&#10;\enumerate@item{&#10;The columns and rows of $\mathbf{A}$ form a basis of $\bbR^{n}$.&#10;}&#10;\enumerate@item{&#10;The $\det(\mathbf{A})\neq 0$.&#10;}&#10;\enumerate@item{&#10;$\mathbf{A}$ is the product of elementary matrices.&#10;}&#10;\end{enumerate}&#10;\end{minipage}&#10;}&#10;\par&#10;\@@section{paragraph}{Sx2.SS0.SSS0.Px1}{}{}{}{Proof of (6).} Consider $%&#10;\mathbf{A}\vec{x}=\vec{0}$. If $\mathbf{A}$ has an inverse $\mathbf{M}$, then&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\vec{0}$\\&#10;$\displaystyle\mathbf{M}\mathbf{A}\vec{x}$&amp;$\displaystyle=\mathbf{M}\vec{0}$\\&#10;$\displaystyle\mathbf{I}\vec{x}$&amp;$\displaystyle=\vec{0}$\\&#10;$\displaystyle\vec{x}$&amp;$\displaystyle=\vec{0}$&#10;\par&#10;\@@section{paragraph}{Sx2.SS0.SSS0.Px2}{}{}{}{Proof of (7).} Suppose $%&#10;\vec{b}\in\bbR^{n}$ is an arbitrary vector. Next consider the system&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\vec{b}$\\&#10;$\displaystyle\mathbf{M}\mathbf{A}\vec{x}$&amp;$\displaystyle=\mathbf{M}\vec{b}$\\&#10;$\displaystyle\vec{x}$&amp;$\displaystyle=\mathbf{M}\vec{b}$&#10;so the system has a unique solution for any $\vec{b}$.&#10;\par&#10;\vspace{1cm}&#10;\par&#10; Let us take a closer look at some of the implications of this theorem. %&#10;Suppose $\mathbf{A}$ is a $3\times 3$ matrix. Suppose each of the following %&#10;systems&#10;\@@amsalign$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix}1\\&#10;0\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;1\\&#10;0\end{bmatrix}$&amp;$\displaystyle\mathbf{A}\vec{x}$&amp;$\displaystyle=\begin{bmatrix%&#10;}0\\&#10;0\\&#10;1\end{bmatrix}$&#10;has a solution, called $\vec{x}_{1},\vec{x}_{2},\vec{x}_{3}$, respectively.&#10;\par&#10; Suppose we want to find the solution of $\mathbf{A}\vec{x}=\begin{%&#10;bmatrix}1\\&#10;e\\&#10;\pi\end{bmatrix}$. Note that&#10;\@@amsalign$\displaystyle\begin{bmatrix}1\\&#10;e\\&#10;\pi\end{bmatrix}$&amp;$\displaystyle=1\begin{bmatrix}1\\&#10;0\\&#10;0\end{bmatrix}+e\begin{bmatrix}0\\&#10;1\\&#10;0\end{bmatrix}+\pi\begin{bmatrix}0\\&#10;0\\&#10;1\end{bmatrix}$\\&#10;$$&amp;$\displaystyle=1\mathbf{A}\vec{x}_{1}+e\mathbf{A}\vec{x}_{2}+\pi\mathbf{A}%&#10;\vec{x}_{3}$\\&#10;$$&amp;$\displaystyle=A\Big(1\vec{x}_{1}+e\vec{x}_{2}+\pi\vec{x}_{3}\Big)$&#10;So the solution is given by $\vec{x}_{1}+e\vec{x}_{2}+\pi\vec{x}_{3}$.&#10;\par&#10;\@@section{section}{Sx3}{}{}{}{Elementary Matrices}&#10;\par&#10; An {elementary matrix}, $\mathbf{E}$, is a matrix we obtain by %&#10;performing exactly one row operation on an identity matrix.&#10;\par&#10; For example, in the $2\times 2$ case,&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{2R_{2}}\begin{bmatrix}1&amp;0\\&#10;0&amp;2\end{bmatrix}$&amp;$$&amp;$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{-3R_{1}+R_{2}}\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}$&amp;$$&amp;$\displaystyle\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{R_{2}\circlearrowright R_{1}}\begin{bmatrix}0&amp;1%&#10;\\&#10;1&amp;0\end{bmatrix}$&#10;The matrices $\begin{bmatrix}1&amp;0\\&#10;0&amp;2\end{bmatrix},\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix},\begin{bmatrix}0&amp;1\\&#10;1&amp;0\end{bmatrix}$ are all elementary matrices.&#10;\par&#10; Consider multiplying a $2\times 2$ matrix $\mathbf{A}$ by the second of %&#10;these elementary matrices:&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}\mathbf{A}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}\begin{bmatrix}a&amp;b\\&#10;c&amp;d\end{bmatrix}$\\&#10;$$&amp;$\displaystyle=\begin{bmatrix}a&amp;b\\&#10;-3a+c&amp;-3b+d\end{bmatrix}$\\&#10;$$\\&#10;$\displaystyle\begin{bmatrix}a&amp;b\\&#10;c&amp;d\end{bmatrix}\xrightarrow[]{-3R_{1}+R_{2}}$&amp;$\displaystyle\begin{bmatrix}a&amp;%&#10;b\\&#10;-3a+c&amp;-3b+d\end{bmatrix}$&#10;We find that multiplying $\mathbf{A}$ by an elementary matrix has the same %&#10;effect as performing the same single row operation on $\mathbf{A}$ as was %&#10;performed to obtain the elementary matrix. This shows us that to perform one %&#10;row operation on a matrix $\mathbf{A}$ we just need to multiply $\mathbf{A}$ %&#10;on the left by an appropriate elementary matrix.&#10;\par&#10;\@@section{subsection}{Sx3.SSx1}{}{}{}{Properties of Inverses}&#10;For a matrix $\mathbf{A}$, the following are true of its inverse $\mathbf{A}^{%&#10;-1}$:&#10;\begin{enumerate}&#10;\enumerate@item{&#10;$\left(\mathbf{A}^{-1}\right)^{-1}=\mathbf{A}$&#10;}&#10;\enumerate@item{&#10;$\left(k\mathbf{A}\right)^{-1}=\frac{1}{k}\mathbf{A}^{-1}$&#10;}&#10;\enumerate@item{&#10;$\mathbf{A}^{-1}$ is unique&#10;}&#10;\enumerate@item{&#10;$\left(\mathbf{A}\mathbf{B}\right)^{-1}=\mathbf{B}^{-1}\mathbf{A}^{-1}$&#10;}&#10;\end{enumerate}&#10;\par&#10;\proof&#10;Suppose $\mathbf{M}_{1}$ and $\mathbf{M}_{2}$ act as inverses of $\mathbf{A}$.%&#10; That is,&#10;\@@amsalign$\displaystyle\mathbf{A}\mathbf{M}_{1}$&amp;$\displaystyle=\mathbf{M}_{%&#10;1}\mathbf{A}$&amp;$\displaystyle=\mathbf{I}$\\&#10;$\displaystyle\mathbf{A}\mathbf{M}_{2}$&amp;$\displaystyle=\mathbf{M}_{2}\mathbf{A%&#10;}$&amp;$\displaystyle=\mathbf{I}$&#10;Note then that&#10;\@@amsalign$\displaystyle\mathbf{M}_{1}$&amp;$\displaystyle=\mathbf{M}_{1}\mathbf{%&#10;I}$\\&#10;$$&amp;$\displaystyle=\mathbf{M}_{1}\left(\mathbf{A}\mathbf{M}_{2}\right)$\\&#10;$$&amp;$\displaystyle=\left(\mathbf{M}_{1}\mathbf{A}\right)\mathbf{M}_{2}$\\&#10;$$&amp;$\displaystyle=\mathbf{I}\mathbf{M}_{2}$&#10;So $\mathbf{M}_{1}=\mathbf{M}_{2}$.&#10;\par&#10;\proof&#10;Assume that $\mathbf{A}$ and $\mathbf{B}$ are both invertible.&#10;\@@amsalign$\displaystyle\left(\mathbf{A}\mathbf{B}\right)\mathbf{B}^{-1}%&#10;\mathbf{A}^{-1}$&amp;$\displaystyle=\mathbf{A}\left(\mathbf{B}\mathbf{B}^{-1}%&#10;\right)\mathbf{A}^{-1}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\mathbf{I}\mathbf{A}^{-1}$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\mathbf{A}^{-1}$\\&#10;$$&amp;$\displaystyle=I$&#10;Similarly we can show that $\mathbf{B}^{-1}\mathbf{A}^{-1}\left(\mathbf{A}%&#10;\mathbf{B}\right)=\mathbf{I}$.&#10;\par&#10;\@@section{paragraph}{Sx3.SSx1.SSS0.Px1}{}{}{}{Question:}Can $\left(%&#10;\mathbf{A}\mathbf{B}\right)$ be invertible while one of $\mathbf{A}$ or $%&#10;\mathbf{B}$ is not invertible? Must one be invertible, must both be invertible%&#10;?&#10;\par&#10; What if $\mathbf{B}$ is not invertible? By the fundamental theorem of %&#10;invertible matrices, we have that $\mathbf{B}\vec{x}=\vec{0}$ must have a non-%&#10;trivial solution, $\vec{x}_{0}$. Then&#10;\@@amsalign$\displaystyle\left(\mathbf{A}\mathbf{B}\right)\vec{x}_{0}$&amp;$%&#10;\displaystyle=\mathbf{A}\left(\mathbf{B}\vec{x}_{0}\right)$\\&#10;$$&amp;$\displaystyle=\mathbf{A}\vec{0}$\\&#10;$$&amp;$\displaystyle=\vec{0}$&#10;Then the equation $\left(\mathbf{A}\mathbf{b}\right)\vec{x}=\vec{0}$ also has %&#10;a non-trivial solution. Thus $\left(\mathbf{A}\mathbf{B}\right)$ cannot be %&#10;invertible.&#10;\par&#10;\@@section{section}{Sx4}{}{}{}{Elementary Matrices and Invertible %&#10;Matrices}&#10;Consider the following matrix $\mathbf{A}$ and its row reduced echelon form:&#10;\@@amsalign$\displaystyle\mathbf{A}=\begin{bmatrix}1&amp;2\\&#10;3&amp;4\end{bmatrix}\xrightarrow[]{-3R_{1}+R_{2}}\begin{bmatrix}1&amp;2\\&#10;0&amp;-2\end{bmatrix}\xrightarrow[]{\frac{-1}{2}R_{2}}\begin{bmatrix}1&amp;2\\&#10;0&amp;1\end{bmatrix}\xrightarrow[]{-2R_{1}+R_{2}}\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}$&#10;By the fundamental theorem of matrices, this shows us that $\mathbf{A}$ is %&#10;invertible.&#10;\par&#10; Note though that we have said that we can represent row operations as %&#10;elementary matrices. Corresponding to these row operations then, are the %&#10;matrices&#10;\@@amsalign$\displaystyle\begin{bmatrix}1&amp;2\\&#10;3&amp;4\end{bmatrix}\underbrace{\xrightarrow[]{-3R_{1}+R_{2}}}_{\mathbf{E}=\begin{%&#10;bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}}\begin{bmatrix}1&amp;2\\&#10;0&amp;-2\end{bmatrix}\underbrace{\xrightarrow[]{\frac{-1}{2}R_{2}}}_{\mathbf{F}=%&#10;\begin{bmatrix}1&amp;0\\&#10;0&amp;\frac{-1}{2}\end{bmatrix}}\begin{bmatrix}1&amp;2\\&#10;0&amp;1\end{bmatrix}\underbrace{\xrightarrow[]{-2R_{1}+R_{2}}}_{\mathbf{G}=\begin{%&#10;bmatrix}1&amp;-2\\&#10;0&amp;1\end{bmatrix}}\begin{bmatrix}1&amp;0\\&#10;0&amp;1\end{bmatrix}$&#10;If we call the matrices between $\mathbf{A}$ and $\mathbf{I}$ as $\mathbf{B}$ %&#10;and $\mathbf{C}$, then we have that&#10;\@@amsalign$\displaystyle\mathbf{B}$&amp;$\displaystyle=\mathbf{E}\mathbf{A}$\\&#10;$\displaystyle\mathbf{C}$&amp;$\displaystyle=\mathbf{F}\mathbf{B}=\mathbf{F}%&#10;\mathbf{E}\mathbf{A}$\\&#10;$\displaystyle\mathbf{I}$&amp;$\displaystyle=\mathbf{G}\mathbf{C}=\mathbf{G}%&#10;\mathbf{F}\mathbf{E}\mathbf{A}$&#10;and so $\left(\mathbf{G}\mathbf{F}\mathbf{E}\right)$ is the inverse of $%&#10;\mathbf{A}$:&#10;\@@amsalign$\displaystyle\left(\mathbf{G}\mathbf{F}\mathbf{E}\right)\mathbf{A}%&#10;$&amp;$\displaystyle=\mathbf{I}$&#10;so&#10;\@@amsalign$\displaystyle\mathbf{A}=\left(\mathbf{G}\mathbf{F}\mathbf{E}\right%&#10;)^{-1}=\mathbf{E}^{-1}\mathbf{F}^{-1}\mathbf{G}^{-1}$&#10;but this assumes that the matrices $\mathbf{E},\mathbf{F},\mathbf{G}$ are %&#10;invertible. This brings us to an important question:&#10;\@@section{paragraph}{Sx4.SSx1.SSS0.Px1}{}{}{}{Question:}Are elementary %&#10;matrices invertible?&#10;In essence, this equates to asking how to reverse a row operation. Every row %&#10;operation can be reversed, and so every elementary matrix is invertible.&#10;\@@amsalign$\displaystyle\mathbf{E}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0\\&#10;-3&amp;1\end{bmatrix}$&amp;$\displaystyle\mathbf{F}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0%&#10;\\&#10;0&amp;\frac{-1}{2}\end{bmatrix}$&amp;$\displaystyle\mathbf{G}$&amp;$\displaystyle=\begin{%&#10;bmatrix}1&amp;-2\\&#10;0&amp;1\end{bmatrix}$\\&#10;$\displaystyle\mathbf{E}^{-1}$&amp;$\displaystyle=\begin{bmatrix}1&amp;0\\&#10;3&amp;0\end{bmatrix}$&amp;$\displaystyle\mathbf{F}^{-1}$&amp;$\displaystyle=\begin{bmatrix%&#10;}1&amp;0\\&#10;0&amp;-2\end{bmatrix}$&amp;$\displaystyle\mathbf{G}^{-1}$&amp;$\displaystyle=\begin{%&#10;bmatrix}1&amp;2\\&#10;0&amp;1\end{bmatrix}$&#10;\par&#10;\@@section{section}{Sx5}{}{}{}{Vector Space}&#10;The term vector space shouldn't be confused for what we know as vectors. %&#10;Consider the set&#10;\@@amsalign$\displaystyle\mathbb{P}_{2}$&amp;$\displaystyle=\text{Polynomials of %&#10;degree $\leq 2$}$&#10;That is, elements of $\mathbb{P}_{2}$ are of the form&#10;\@@amsalign$\displaystyle ax^{2}+bx+c$&#10;where $a,b,c\in\bbR$.&#10;\par&#10; Let $u,v\in\mathbb{P}_{2}$. It is apparent then that $u(x)+v(x)\in%&#10;\mathbb{P}_{2}$ and for any $k\in\bbR$, $ku(x)\in\mathbb{P}_{2}$.&#10;\par&#10; This is very similar to $\bbR^{3}$. Consider $\vec{u},\vec{v}\in\bbR^{3}%&#10;$. Then $\vec{u}+\vec{v}\in\bbR^{3}$ and for any $k\in\bbR^{3}$, $k\vec{u}\in%&#10;\bbR^{3}$.&#10;\par&#10; In fact, many things besides standard vectors will be considered as %&#10;vectors. Polynomials, functions, and matrices can all be seen as vectors in %&#10;their respective vector spaces.&#10;\par&#10;\par&#10;\end{document}" display="block">
  <mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>k</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>4</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>f</mi>
      <mi>i</mi>
      <mi>l</mi>
      <mi>l</mi>
      <mo>=</mo>
      <mi>b</mi>
      <mi>l</mi>
      <mi>a</mi>
      <mi>c</mi>
      <mi>k</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\tikzstyle</mtext>
    </merror>
    <mi>b</mi>
    <mi>l</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mi>n</mi>
    <mi>o</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mo>=</mo>
    <mrow>
      <mo stretchy="false">[</mo>
      <mi>c</mi>
      <mi>i</mi>
      <mi>r</mi>
      <mi>c</mi>
      <mi>l</mi>
      <mi>e</mi>
      <mo>,</mo>
      <mi>i</mi>
      <mi>n</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>r</mi>
      <mi>s</mi>
      <mi>e</mi>
      <mi>p</mi>
      <mo>=</mo>
      <mn>3</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>m</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>i</mi>
      <mi>m</mi>
      <mi>u</mi>
      <mi>m</mi>
      <mi>s</mi>
      <mi>i</mi>
      <mi>z</mi>
      <mi>e</mi>
      <mo>=</mo>
      <mn>8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo>,</mo>
      <mi>d</mi>
      <mi>r</mi>
      <mi>a</mi>
      <mi>w</mi>
      <mo>,</mo>
      <mi>l</mi>
      <mi>i</mi>
      <mi>n</mi>
      <mi>e</mi>
      <mi>w</mi>
      <mi>i</mi>
      <mi>d</mi>
      <mi>t</mi>
      <mi>h</mi>
      <mo>=</mo>
      <mn>0.8</mn>
      <mi>p</mi>
      <mi>t</mi>
      <mo stretchy="false">]</mo>
    </mrow>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\geometry</mtext>
    </merror>
    <mi>l</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>p</mi>
    <mi>a</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>w</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mo>=</mo>
    <mn>17</mn>
    <mi>c</mi>
    <mi>m</mi>
    <mo>,</mo>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>h</mi>
    <mi>t</mi>
    <mo>=</mo>
    <mn>22</mn>
    <mi>c</mi>
    <mi>m</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>a</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>w</mi>
    <mi>s</mi>
    <merror class="ltx_ERROR undefined undefined">
      <mtext>\usetikzlibrary</mtext>
    </merror>
    <mi>p</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>k</mi>
    <mi>s</mi>
    <mtext xml:id="Sx1">RecallPreviously, we saw the inverse 𝐌 of a matrix 𝐀:𝐀𝐌=𝐌𝐀=𝐈n𝐀𝐌=𝐌𝐀=𝐈nwhere 𝐀 is an n×n matrix; that is, 𝐀 must be square to have an inverse.In a 3×3 matrix, this equates to solving each of𝐀⁢x→=[100]𝐀⁢x→=[100]𝐀⁢x→=[010]𝐀⁢x→=[010]𝐀⁢x→=[001]𝐀⁢x→=[001]which can be formulated as the process of transforming an extended augmented matrix through row reductions:[[c|c]𝐀𝐈]⟶[[c|c]𝐈𝐌][[c|c]𝐀𝐈]⟶[[c|c]𝐈𝐌]Fundamental Theorem of Invertible Matrices
Let 𝐀n×n be a matrix over a field 𝔽. Then the following are equivalent (TFAE):1.𝐀 is invertible.2.The reduced row echelon form (rref) of 𝐀 is 𝐈.3.The rank⁢(A)=n.4.The columns of 𝐀 are linearly independent.5.The rows of 𝐀 are linearly independent.6.The homogeneous system 𝐀⁢x→=0→ has only the zero solution x→=0→.7.𝐀⁢x→=b→ will have a unique solution for all b→∈\bbRn.8.The span of the columns of 𝐀=\bbRn.9.The columns and rows of 𝐀 form a basis of \bbRn.10.The det⁡(𝐀)≠0.11.𝐀 is the product of elementary matrices.
Proof of (6).Consider 𝐀⁢x→=0→. If 𝐀 has an inverse 𝐌, then𝐀⁢x→=0→𝐀⁢x→=0→𝐌𝐀⁢x→=𝐌⁢0→𝐌𝐀⁢x→=𝐌⁢0→𝐈⁢x→=0→𝐈⁢x→=0→x→=0→x→=0→Proof of (7).Suppose b→∈\bbRn is an arbitrary vector. Next consider the system𝐀⁢x→=b→𝐀⁢x→=b→𝐌𝐀⁢x→=𝐌⁢b→𝐌𝐀⁢x→=𝐌⁢b→x→=𝐌⁢b→x→=𝐌⁢b→so the system has a unique solution for any b→.Let us take a closer look at some of the implications of this theorem. Suppose 𝐀 is a 3×3 matrix. Suppose each of the following systems𝐀⁢x→=[100]𝐀⁢x→=[100]𝐀⁢x→=[010]𝐀⁢x→=[010]𝐀⁢x→=[001]𝐀⁢x→=[001]has a solution, called x→1,x→2,x→3, respectively.Suppose we want to find the solution of 𝐀⁢x→=[1eπ]. Note that[1eπ]=1⁢[100]+e⁢[010]+π⁢[001][1eπ]=1⁢[100]+e⁢[010]+π⁢[001]=1⁢𝐀⁢x→1+e⁢𝐀⁢x→2+π⁢𝐀⁢x→3=1⁢𝐀⁢x→1+e⁢𝐀⁢x→2+π⁢𝐀⁢x→3=A⁢(1⁢x→1+e⁢x→2+π⁢x→3)=A⁢(1⁢x→1+e⁢x→2+π⁢x→3)So the solution is given by x→1+e⁢x→2+π⁢x→3.Elementary MatricesAn elementary matrix, 𝐄, is a matrix we obtain by performing exactly one row operation on an identity matrix.For example, in the 2×2 case,[1001]→2⁢R2[1002][1001]→2⁢R2[1002][1001]→-3⁢R1+R2[10-31][1001]→-3⁢R1+R2[10-31][1001]→R2⁢\circlearrowright⁢R1[0110][1001]→R2⁢\circlearrowright⁢R1[0110]The matrices [1002],[10-31],[0110] are all elementary matrices.Consider multiplying a 2×2 matrix 𝐀 by the second of these elementary matrices:[10-31]⁢𝐀=[10-31]⁢[abcd][10-31]⁢𝐀=[10-31]⁢[abcd]=[ab-3⁢a+c-3⁢b+d]=[ab-3⁢a+c-3⁢b+d][abcd]→-3⁢R1+R2[ab-3⁢a+c-3⁢b+d][abcd]→-3⁢R1+R2[ab-3⁢a+c-3⁢b+d]We find that multiplying 𝐀 by an elementary matrix has the same effect as performing the same single row operation on 𝐀 as was performed to obtain the elementary matrix. This shows us that to perform one row operation on a matrix 𝐀 we just need to multiply 𝐀 on the left by an appropriate elementary matrix.Properties of InversesFor a matrix 𝐀, the following are true of its inverse 𝐀-1:1.(𝐀-1)-1=𝐀2.(k⁢𝐀)-1=1k⁢𝐀-13.𝐀-1 is unique4.(𝐀𝐁)-1=𝐁-1⁢𝐀-1{proof}Suppose 𝐌1 and 𝐌2 act as inverses of 𝐀. That is,𝐀𝐌1=𝐌1⁢𝐀𝐀𝐌1=𝐌1⁢𝐀=𝐈=𝐈𝐀𝐌2=𝐌2⁢𝐀𝐀𝐌2=𝐌2⁢𝐀=𝐈=𝐈Note then that𝐌1=𝐌1⁢𝐈𝐌1=𝐌1⁢𝐈=𝐌1⁢(𝐀𝐌2)=𝐌1⁢(𝐀𝐌2)=(𝐌1⁢𝐀)⁢𝐌2=(𝐌1⁢𝐀)⁢𝐌2=𝐈𝐌2=𝐈𝐌2So 𝐌1=𝐌2.{proof}Assume that 𝐀 and 𝐁 are both invertible.(𝐀𝐁)⁢𝐁-1⁢𝐀-1=𝐀⁢(𝐁𝐁-1)⁢𝐀-1(𝐀𝐁)⁢𝐁-1⁢𝐀-1=𝐀⁢(𝐁𝐁-1)⁢𝐀-1=𝐀𝐈𝐀-1=𝐀𝐈𝐀-1=𝐀𝐀-1=𝐀𝐀-1=I=ISimilarly we can show that 𝐁-1⁢𝐀-1⁢(𝐀𝐁)=𝐈.Question:Can (𝐀𝐁) be invertible while one of 𝐀 or 𝐁 is not invertible? Must one be invertible, must both be invertible?What if 𝐁 is not invertible? By the fundamental theorem of invertible matrices, we have that 𝐁⁢x→=0→ must have a non-trivial solution, x→0. Then(𝐀𝐁)⁢x→0=𝐀⁢(𝐁⁢x→0)(𝐀𝐁)⁢x→0=𝐀⁢(𝐁⁢x→0)=𝐀⁢0→=𝐀⁢0→=0→=0→Then the equation (𝐀𝐛)⁢x→=0→ also has a non-trivial solution. Thus (𝐀𝐁) cannot be invertible.Elementary Matrices and Invertible MatricesConsider the following matrix 𝐀 and its row reduced echelon form:𝐀=[1234]→-3⁢R1+R2[120-2]→-12⁢R2[1201]→-2⁢R1+R2[1001]𝐀=[1234]→-3⁢R1+R2[120-2]→-12⁢R2[1201]→-2⁢R1+R2[1001]By the fundamental theorem of matrices, this shows us that 𝐀 is invertible.Note though that we have said that we can represent row operations as elementary matrices. Corresponding to these row operations then, are the matrices[1234]⁢→-3⁢R1+R2⏟𝐄=[10-31]⁢[120-2]⁢→-12⁢R2⏟𝐅=[100-12]⁢[1201]⁢→-2⁢R1+R2⏟𝐆=[1-201]⁢[1001][1234]⁢→-3⁢R1+R2⏟𝐄=[10-31]⁢[120-2]⁢→-12⁢R2⏟𝐅=[100-12]⁢[1201]⁢→-2⁢R1+R2⏟𝐆=[1-201]⁢[1001]If we call the matrices between 𝐀 and 𝐈 as 𝐁 and 𝐂, then we have that𝐁=𝐄𝐀𝐁=𝐄𝐀𝐂=𝐅𝐁=𝐅𝐄𝐀𝐂=𝐅𝐁=𝐅𝐄𝐀𝐈=𝐆𝐂=𝐆𝐅𝐄𝐀𝐈=𝐆𝐂=𝐆𝐅𝐄𝐀and so (𝐆𝐅𝐄) is the inverse of 𝐀:(𝐆𝐅𝐄)⁢𝐀=𝐈(𝐆𝐅𝐄)⁢𝐀=𝐈so𝐀=(𝐆𝐅𝐄)-1=𝐄-1⁢𝐅-1⁢𝐆-1𝐀=(𝐆𝐅𝐄)-1=𝐄-1⁢𝐅-1⁢𝐆-1but this assumes that the matrices 𝐄,𝐅,𝐆 are invertible. This brings us to an important question:Question:Are elementary matrices invertible?
In essence, this equates to asking how to reverse a row operation. Every row operation can be reversed, and so every elementary matrix is invertible.𝐄=[10-31]𝐄=[10-31]𝐅=[100-12]𝐅=[100-12]𝐆=[1-201]𝐆=[1-201]𝐄-1=[1030]𝐄-1=[1030]𝐅-1=[100-2]𝐅-1=[100-2]𝐆-1=[1201]𝐆-1=[1201]Vector SpaceThe term vector space shouldn’t be confused for what we know as vectors. Consider the setℙ2=Polynomials of degree ≤2ℙ2=Polynomials of degree ≤2That is, elements of ℙ2 are of the forma⁢x2+b⁢x+ca⁢x2+b⁢x+cwhere a,b,c∈\bbR.Let u,v∈ℙ2. It is apparent then that u⁢(x)+v⁢(x)∈ℙ2 and for any k∈\bbR, k⁢u⁢(x)∈ℙ2.This is very similar to \bbR3. Consider u→,v→∈\bbR3. Then u→+v→∈\bbR3 and for any k∈\bbR3, k⁢u→∈\bbR3.In fact, many things besides standard vectors will be considered as vectors. Polynomials, functions, and matrices can all be seen as vectors in their respective vector spaces.</mtext>
  </mrow>
</math>
